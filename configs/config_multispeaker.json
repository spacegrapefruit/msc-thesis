{
    "model": "tacotron2",
    "run_name": "Tacotron2-DDC",
    "project_name": "MSc-TTS-Thesis",
    // AUDIO PARAMETERS
    "audio": {
        // stft parameters
        "fft_size": 1024, // number of stft frequency levels. Size of the linear spectogram frame.
        "win_length": 1024, // stft window length in ms.
        "hop_length": 256, // stft window hop-lengh in ms.
        "frame_length_ms": null, // stft window length in ms.If null, 'win_length' is used.
        "frame_shift_ms": null, // stft window hop-lengh in ms. If null, 'hop_length' is used.
        "stft_pad_mode": "reflect",
        // Audio processing parameters
        "sample_rate": 22050, // DATASET-RELATED: wav sample-rate.
        "resample": false,
        "preemphasis": 0.0, // pre-emphasis to reduce spec noise and make it more structured. If 0.0, no -pre-emphasis.
        "ref_level_db": 20, // reference level db, theoretically 20db is the sound of air.
        "do_sound_norm": false,
        "log_func": "np.log",
        // Silence trimming
        "do_trim_silence": true, // enable trimming of slience of audio as you load it. LJspeech (true), TWEB (false), Nancy (true)
        "trim_db": 60, // threshold for timming silence. Set this according to your dataset.
        "do_rms_norm": false,
        "db_level": null,
        // Griffin-Lim
        "power": 1.5, // value to sharpen wav signals after GL algorithm.
        "griffin_lim_iters": 60, // #griffin-lim iterations. 30-60 is a good range. Larger the value, slower the generation.
        // MelSpectrogram parameters
        "num_mels": 80, // size of the mel spec frame.
        "mel_fmin": 0.0, // minimum freq level for mel-spec. ~50 for male and ~95 for female voices. Tune for dataset!!
        "mel_fmax": 8000.0, // maximum freq level for mel-spec. Tune for dataset!!
        "spec_gain": 1,
        // Normalization parameters
        "do_amp_to_db_linear": true,
        "do_amp_to_db_mel": true,
        "pitch_fmax": 640.0,
        "pitch_fmin": 1.0,
        "signal_norm": false, // normalize spec values. Mean-Var normalization if 'stats_path' is defined otherwise range normalization defined by the other params.
        "min_level_db": -100, // lower bound for normalization
        "symmetric_norm": true, // move normalization to range [-1, 1]
        "max_norm": 4.0, // scale normalization to range [-max_norm, max_norm] or [0, max_norm]
        "clip_norm": true, // clip normalized values into the range.
        "stats_path": null // DO NOT USE WITH MULTI_SPEAKER MODEL. scaler stats file computed by 'compute_statistics.py'. If it is defined, mean-std based notmalization is used and other normalization params are ignored
    },
    // VOCABULARY PARAMETERS
    "characters": {
        "pad": "",
        "eos": "",
        "bos": "",
        "characters": "aąbcčdeęėfghiįyjklmnoprsštuųūvzž .,-",
        "punctuations": " .,-",
        "phonemes": ""
    },
    // DISTRIBUTED TRAINING
    "reinit_layers": [], // give a list of layer names to restore from the given checkpoint. If not defined, it reloads all heuristically matching layers.
    // TRAINING
    "batch_size": 64, // Batch size for training. Lower values than 32 might cause hard to learn attention. It is overwritten by 'gradual_training'.
    "eval_batch_size": 64,
    "r": 1, // Number of decoder frames to predict per iteration. Set the initial values if gradual training is enabled.
    "gradual_training": null, //set gradual training steps [first_step, r, batch_size]. If it is null, gradual training is disabled. For Tacotron, you might need to reduce the 'batch_size' as you proceeed.
    "mixed_precision": false, // level of optimization with NVIDIA's apex feature for automatic mixed FP16/FP32 precision (AMP), NOTE: currently only O1 is supported, and use "O1" to activate.
    "start_by_longest": false,
    "shuffle": false,
    "drop_last": false,
    // LOSS SETTINGS
    "loss_masking": true, // enable / disable loss masking against the sequence padding.
    "decoder_loss_alpha": 1, // original decoder loss weight. If > 0, it is enabled
    "postnet_loss_alpha": 1, // original postnet loss weight. If > 0, it is enabled
    "postnet_diff_spec_alpha": 0, // differential spectral loss weight. If > 0, it is enabled
    "decoder_diff_spec_alpha": 0, // differential spectral loss weight. If > 0, it is enabled
    "decoder_ssim_alpha": 0, // decoder ssim loss weight. If > 0, it is enabled
    "postnet_ssim_alpha": 0, // postnet ssim loss weight. If > 0, it is enabled
    "ga_alpha": 5.0, // weight for guided attention loss. If > 0, guided attention is enabled.
    "stopnet_pos_weight": 15.0, // pos class weight for stopnet loss since there are way more negative samples than positive samples.
    // VALIDATION
    "run_eval": true,
    "eval_split_max_size": null,
    "eval_split_size": 0.01,
    "test_delay_epochs": 50, //Until attention is aligned, testing only wastes computation time.
    "test_sentences": [
        "aš labai mėgstu skaityti knygas.",
        "šiandien oras yra puikus idealiai tinkantis pasivaikščiojimams gamtoje.",
        "technologijos sparčiai keičia mūsų kasdienį gyvenimą ir darbo būdus.",
        "muzika turi galią sujungti žmones iš skirtingų kultūrų ir kalbų.",
        "kelionės atveria protą naujoms idėjoms ir patirtims."
    ],
    "training_seed": 54321,
    // OPTIMIZER
    "grad_clip": 1.0, // upper limit for gradients for clipping.
    "epochs": 400, // total number of epochs to train.
    "lr": 0.001, // Initial learning rate. If Noam decay is active, maximum learning rate.
    "lr_scheduler": "MultiStepLR",
    "lr_scheduler_params": {
        "milestones": [
            10000,
            20000,
            40000
        ],
        "gamma": 0.2
    },
    "scheduler_after_epoch": false,
    "wd": 0.000001, // Weight decay weight.
    // "warmup_steps": 4000, // Noam decay steps to increase the learning rate from 0 to "lr"
    "seq_len_norm": false, // Normalize eash sample loss with its length to alleviate imbalanced datasets. Use it if your dataset is small or has skewed distribution of sequence lengths.
    "use_grad_scaler": false,
    // TACOTRON PRENET
    "memory_size": -1, // ONLY TACOTRON - size of the memory queue used fro storing last decoder predictions for auto-regression. If < 0, memory queue is disabled and decoder only uses the last prediction frame.
    "prenet_type": "original", // "original" or "bn".
    "prenet_dropout": true, // enable/disable dropout at prenet.
    "prenet_dropout_at_inference": true,
    // TACOTRON ATTENTION
    "attention_type": "original", // 'original' , 'graves', 'dynamic_convolution'
    "attention_heads": 4, // number of attention heads (only for 'graves')
    "attention_norm": "softmax", // softmax or sigmoid.
    "attention_win": false,
    "windowing": false, // Enables attention windowing. Used only in eval mode.
    "use_forward_attn": false, // if it uses forward attention. In general, it aligns faster.
    "forward_attn_mask": false, // Additional masking forcing monotonicity only in eval mode.
    "transition_agent": false, // enable/disable transition agent of forward attention.
    "location_attn": true, // enable_disable location sensitive attention. It is enabled for TACOTRON by default.
    "bidirectional_decoder": false, // use https://arxiv.org/abs/1907.09006. Use it, if attention does not work well with your dataset.
    "double_decoder_consistency": true, // use DDC explained here https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency-draft/
    "ddc_r": 7, // reduction rate for coarse decoder.
    // STOPNET
    "stopnet": true, // Train stopnet predicting the end of synthesis.
    "separate_stopnet": false, // Train stopnet seperately if 'stopnet==true'. It prevents stopnet loss to influence the rest of the model. It causes a better model, but it trains SLOWER.
    "max_decoder_steps": 2000,
    "speakers_file": "data/processed/tts_dataset_liepa2_multispeaker/speakers.json",
    // TENSORBOARD and LOGGING
    "print_step": 25, // Number of steps to log training on console.
    "plot_step": 100, // Number of steps to plot TB training figures.
    "print_eval": false, // If True, it prints intermediate loss values in evalulation.
    "save_step": 2000,
    "save_n_checkpoints": 5,
    "save_checkpoints": true, // If true, it saves checkpoints per "save_step"
    "save_all_best": false, // If true, saves all best_models after keep_after steps
    "save_best_after": 0, // always save
    "model_param_stats": false, // true, plots param stats per layer on tensorboard. Might be memory consuming, but good for debugging.
    // DATA LOADING
    "enable_eos_bos_chars": false, // enable/disable beginning of sentence and end of sentence chars.
    "num_loader_workers": 8, // number of training data loader processes. Don't set it too big. 4-8 are good values.
    "num_eval_loader_workers": 8, // number of evaluation data loader processes.
    "precompute_num_workers": 8,
    "batch_group_size": 4, //Number of batches to shuffle after bucketing.
    "use_noise_augment": false,
    "add_blank": false,
    "min_audio_len": 1,
    "max_audio_len": Infinity,
    "min_text_len": 1,
    "max_text_len": Infinity,
    // PATHS
    "output_path": "training_output",
    // PHONEMES
    "use_phonemes": false, // use phonemes instead of raw characters. It is suggested for better pronounciation.
    // MULTI-SPEAKER and GST
    "use_speaker_embedding": true, // use speaker embedding to enable multi-speaker learning.
    "use_gst": false, // use global style tokens
    "speaker_embedding_dim": 512,
    "num_speakers": 20,
    // COMPUTE FEATURES
    "compute_f0": false,
    "compute_energy": false,
    "compute_linear_spec": false,
    // DATASETS
    "datasets": // List of datasets. They all merged and they get different speaker_ids.
    [
        {
            "name": "liepa2_lt_multispeaker",
            "path": "data/processed/tts_dataset_liepa2_multispeaker/",
            "meta_file_train": "metadata.csv",
            "meta_file_val": null,
            "formatter": "ljspeech_multispeaker",
            "language": "lt"
        }
    ]
}