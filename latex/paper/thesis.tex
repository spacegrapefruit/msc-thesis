%%%%%
%%%%%  Use LUALATEX, not LATEX.
%%%%%
%%%%
\documentclass[]{VUMIFTemplateClass}

\usepackage{indentfirst}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{mathtools}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage[hidelinks]{hyperref}
\usepackage{color,algorithm,algorithmic}
\usepackage[nottoc]{tocbibind}
\usepackage{tocloft}

\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage}

\makeatletter
\renewcommand{\fnum@algorithm}{\thealgorithm}
\makeatother
\renewcommand\thealgorithm{\arabic{algorithm} algorithm}

\usepackage{biblatex}
\bibliography{bibliografija}
%% to change the numbering (numeric or alphabetic) of bibliographic sources, make the change in VUMIFTemplateClass.cls, line 139

% Author's MACROS
\newcommand{\EE}{\mathbb{E}\,} % Mean
\newcommand{\ee}{{\mathrm e}}  % nice exponent
\newcommand{\RR}{\mathbb{R}}

\studyprogramme{Data Science}
\worktype{Master's thesis}
\worktitle{Data Selection Strategies for Multi-Speaker Text-to-Speech Synthesis in Lithuanian}
\secondworktitle{Work Title in Lithuanian}
\workauthor{Aleksandr Jan Smoliakov}

\supervisor{Gerda Ana Melnik-Leroy}
\reviewer{pedagogical/scientific title Name Surname}
%If present, otherwise delete
\scientificadvisor{pedagogical/scientific title Name Surname}
%If present, otherwise delete

\begin{document}
\selectlanguage{english}

\onehalfspacing
\input{TitlePage}

% %% TODO Acknowledgements Section
% \sectionnonumnocontent{Acknowledgements}
% The author is thankful the Information Technology Research Center, Faculty of Mathematics and Informatics, Vilnius University, for providing the High-Performance Computing (HPC) resources for this research.
% %%
% %%
% %%      If you have used IT resources (CPU-h, GPU-h, other IT resources) provided by MIF for your thesis research, please leave the acknowledgement; if you have not, you can delete it.
% %%
% %%

% You can also add here acknowledgements for various other things, such as your supervisor, university, company, etc.

\singlespacing
\selectlanguage{english}

% list of figures, delete if not needed
\listoffigures

% list of tables, delete if not needed
\listoftables

% Table of contents
\tableofcontents
\onehalfspacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\sectionnonum{Introduction}

The goal of creating machines that can speak like humans has captivated
researchers for centuries. One of the earliest known attempts dates back to the
18th century, with Wolfgang von Kempelen's mechanical speech machine that
utilized a bellows-driven lung and physical models of the tongue and lips.

Over the centuries, advancements in technology and understanding of human
speech have driven significant progress in this field. Today's state-of-the-art
systems, dominated by end-to-end (E2E) neural models, have achieved highly
naturalistic speech with unprecedented acoustic quality. Notably, these
end-to-end systems have unified the entire synthesis process into a single
neural network, eliminating the need for complex multi-stage pipelines.

% \subsection{Background and motivation}

% \subsection{Problem statement}

Training high-quality TTS models typically requires large amounts of annotated
speech data. The common recommendation is to use at least 10~hours of recorded
speech from a single speaker to achieve good results.

Liepa~2 is a recently released Lithuanian speech corpus that contains
1000~hours of annotated speech; however, this data is distributed across more
than 2600~speakers, with most speakers contributing only a few minutes of
speech. The top speaker has around 2.5~hours of recorded speech.

Training a high-quality single-speaker TTS model on such limited data poses a
challenge. Multi-speaker TTS models can utilize data from multiple speakers to
improve performance. However, training on all available data is a
time-consuming and computationally expensive process, especially in the context
of a master's thesis.

Therefore, it makes sense to explore strategies for selecting smaller subsets
of the available data for training. The question that arises is, what is the
best way to sample multi-speaker data for training TTS models?

% \subsection{Research questions}

This thesis aims to answer the following research questions:

\begin{itemize}
      \item TODO
\end{itemize}

% \subsection{Objectives}

% \subsection{Scope of the study}

Scope: This study is exclusively focused on the Lithuanian language and the
Liepa~2 speech corpus. It investigates a fixed total training data size of
30~hours. The models are limited to Tacotron~2 with DDC and FastPitch
architectures within the Coqui TTS framework, using a pre-trained WaveGlow
vocoder for waveform generation.

Limitations: The findings may not generalize to other languages, datasets with
different characteristics, or other TTS architectures. The 30-hour training
data size is a practical constraint and may not reflect performance at larger
scales.

% \subsection{Thesis structure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Literature review
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Literature review}

\subsection{Digital representation of audio}

Speech, or sound in general, is a continuous pressure wave that propagates
through a medium, such as air. The key properties of sound waves include
frequency (pitch), amplitude (loudness), and phase.

Converting continuous sound waves into a digital format suitable for computer
processing involves two main steps: sampling and quantization.

Sampling is the process of measuring the amplitude of the sound wave at regular
time intervals. The rate at which these samples are taken is called the
sampling rate. According to the Nyquist-Shannon~\cite{shannon1949communication}
sampling theorem, accurate reconstruction of a continuous signal requires a
sampling rate that is at least twice the highest frequency present in the
signal. The majority of human speech information is concentrated between 100~Hz
and 8~kHz.~\cite{rabiner2010theory} Thus, in text-to-speech applications,
common sampling rates for audio are 22.05~kHz and 24~kHz, which can capture
frequencies up to approx. 11~kHz and 12~kHz, respectively.

Quantization (also known as bit depth) is the mapping of continuous amplitude
values to discrete levels for digital representation. Typical bit depths for
audio are 16-bit and 24-bit formats.

Pre-emphasis is a high-frequency filtering technique applied to audio signals
before further processing. Natural speech signals tend to have more energy in
the lower frequencies, with a gradual drop-off towards higher frequencies
(typically around -6~dB per octave). Pre-emphasis compensates for this spectral
tilt by boosting high frequencies using a first-order high-pass filter, which
is defined as:

\begin{equation}
      y[n] = x[n] - \alpha x[n-1]
\end{equation}

where \( y[n] \) is the pre-emphasized signal, \( x[n] \) is the original
signal, \( \alpha \) is the pre-emphasis coefficient (typically between 0.9 and
1.0), and \( n \) is the sample index.

This transformation balances the frequency spectrum, improving the
signal-to-noise ratio for higher frequencies and preventing the model from
optimizing only for low-frequency components.

\subsection{Time-Frequency Analysis}

\subsubsection{Fourier Transform}

Fourier Transform (FT) is a mathematical technique that transforms a
time-domain signal (such as an audio waveform) into its frequency-domain
representation. The signal is decomposed into a sum of sine and cosine waves at
various frequencies, each with a specific amplitude and phase. This allows us
to analyze the frequency content of the signal.

Short-Time Fourier Transform~\cite{oppenheim1999discrete} (STFT) extends the FT
by applying it to short, overlapping segments (frames) of the signal. This
transformation provides a time-frequency representation, showing how the
frequency content of the signal changes over time.

In TTS applications, the STFT is computed by dividing the audio signal into
short frames (usually, 20-40~ms) with a certain overlap (usually, 50-75\%)
between frames.

In order to reduce spectral leakage and improve frequency resolution, each
frame is multiplied by a window function, such as a Hamming or Hann window.

\subsubsection{Spectrogram}

The spectrogram is a visual representation of the STFT, displaying frequency on
the vertical axis, time on the horizontal axis, and amplitude represented by
the color intensity.

\begin{figure}[ht]
      \centering
      % TODO \includegraphics[width=\textwidth]{spectrogram_example}
      \caption{Comparison of a linear spectrogram vs.\ a Mel-spectrogram.}
      \label{fig:mel_spec}
\end{figure}

\subsubsection{Mel-spectrogram}

The human ear does not perceive frequencies linearly --- it is more sensitive
to lower frequencies than higher ones. To mimic this perceptual characteristic,
the Mel scale~\cite{stevens1937scale} maps linear frequency \( f \) (in Hz) to
a perceptual scale \( m \) (in Mels) using the following formula:

\begin{equation}
      m = 2595 \cdot \log_{10}\left(1 + \frac{f}{700}\right)
\end{equation}

Mel-spectrograms are computed by applying a Mel filterbank of overlapping
triangular filters (or kernels) to the magnitude spectrogram obtained from the
STFT. This results in a compressed representation of the audio signal that
aligns more closely with human auditory perception. Such Mel-spectrograms are
commonly used as input features for modern TTS systems.

\subsection{Text-to-speech synthesis}

Text-to-Speech (TTS) synthesis, also known as speech synthesis, is the process
of converting written text into human-like spoken words. Nowadays TTS is a key
technology in numerous applications, including virtual assistants,
accessibility tools, and language learning platforms.

\subsubsection{Traditional TTS approaches}

Early attempts at artificial speech synthesis evolved from the first mechanical
devices in the 18th century to electronic systems. Wolfgang von Kempelen's
mechanical speech machine demonstrated basic phoneme production using a
physical model of the vocal tract. In 1937, Homer Dudley's invention of the
Voder~\cite{klatt1987review} became the first electronic speech synthesizer
that could produce intelligible speech through operator-controlled acoustic
parameters, establishing the foundation for modern electronic synthesis
methods.

In the decades that followed, two main approaches for speech synthesis emerged:
concatenative synthesis and parametric synthesis.

\subsubsection{Concatenative synthesis}

The concatenative synthesis approach~\cite{hunt1996unit} synthesizes speech by
piecing together pre-recorded segments of human speech. This method involves
several steps. First, it requires pre-recording a large database of speech
segments spoken by a human voice actor in pristine, highly controlled studio
conditions to ensure consistent audio quality and minimize background noise.
Each segment is labeled and indexed based on its phonetic and prosodic
properties.

During synthesis, the system breaks down the input text into short linguistic
units (such as phonemes or syllables) using a text analysis module. Then, it
queries the speech database to find the best-matching segments for each unit
using selection cost functions~\cite{black1995optimising}. The retrieved
segments are blended and concatenated to form a continuous speech waveform. The
system then uses signal processing techniques to smooth the transitions between
segments and adjust pitch and duration to match the desired output
characteristics.

Concatenative synthesis can produce natural-sounding individual speech units,
but the final audio often has noticeable glitches and breaks at the points
where segments are joined together~\cite{black1995optimising}. The segments may
not blend smoothly due to differences in pitch, duration, and timbre. The
prosody also tends to sound ``choppy'' and unnatural, since stringing
disjointed segments together does not capture the natural rhythm and intonation
patterns of connected speech.

Finally, concatenative synthesis requires language-specific expertise to design
and maintain the underlying speech database and selection algorithms. This need
for extensive data can make it challenging to develop concatenative TTS systems
for low-resource languages or dialects.

\subsubsection{Parametric synthesis}

In contrast, statistical parametric speech synthesis~\cite{zen2009statistical}
(SPSS) uses statistical models, typically Hidden Markov Models
(HMMs)~\cite{tokuda2013speech}, to generate the parameters that control a
speech waveform.

This method involves training a statistical model on a large corpus of recorded
speech. The model learns the relationship between linguistic features (like
phonemes and prosody) and the acoustic features of the speech signal, such as
spectral envelope and fundamental frequency. During synthesis, the system takes
text as input, converts it to a sequence of linguistic features, and then uses
the trained model to generate a corresponding sequence of acoustic parameters.

Compared to concatenative synthesis, the statistical approach allows for more
flexibility and control over the speech synthesis process, enabling the
generation of a wider variety of voices and speaking styles. However, HMM-based
synthesis~\cite{tokuda2013speech} had a persistent problem: the statistical
averaging built into the models tended to over-smooth the acoustic features,
creating a characteristic ``buzzy'' or ``muffled'' sound that lacked the
sharpness and detail of natural human speech.

\subsection{Linguistic Representation (Text Processing)}

\subsubsection{Text normalization}

Text normalization~\cite{sproat2001normalization} is the process of converting
raw text into a standardized format suitable for TTS synthesis. Typical steps
include:

\begin{itemize}
      \item Lowercasing: Converting all text to lowercase to reduce variability.
      \item Punctuation removal: Stripping out unnecessary punctuation marks.
      \item Expanding abbreviations: Converting abbreviations to their full forms (e.g.,
            ``Dr.'' -> ``Doctor'').
      \item Number normalization: Converting numbers into their spoken equivalents (e.g.,
            ``123'' -> ``one hundred twenty-three'').
      \item Tokenization: Splitting text into words or subword units.
\end{itemize}

\subsubsection{Graphemes vs. Phonemes}

A key decision in TTS systems is the choice of input representation for text.

There are two main approaches, namely grapheme-based and phoneme-based. The
grapheme-based approach uses the raw characters of the written text as input to
the TTS model. This method is straightforward and does not require any
additional pre-processing steps. However, it can struggle with languages that
have complex orthographies or inconsistent spelling-to-sound correspondences.

In contrast, the phoneme-based approach uses a phonetic transcription of the
text, typically in the International Phonetic Alphabet (IPA) format. The
underlying assumption is that phonemes possess a more direct and consistent
mapping to the acoustic features of speech, making it easier for the TTS model
to learn the relationship between text and audio. However, this approach
requires an additional grapheme-to-phoneme (G2P) conversion
step~\cite{bisani2008joint}, which requires a complex pre-processing pipeline
or a separate G2P model.

There is yet another approach that combines both grapheme and phoneme inputs,
known as accented grapheme-based synthesis. This method augments the raw text
with accentuation markers (typically tilde, acute, grave accents) to indicate
the stressed syllables in words. This additional information provides cues
about the prosodic features of speech, which can help improve the pronunciation
of speech.

\subsubsection{Specific challenges in Lithuanian}

Lithuanian is a Baltic language with a rich inflectional morphology and complex
prosodic structure. It is a pitch-accent language with free stress, meaning the
stress can fall on any syllable in a word, and can change the position
depending on the grammatical form.

Challenges in Lithuanian TTS synthesis include:

\begin{itemize}
      \item \textbf{High OOV rate:} Due to extensive word inflection, the number of unique word forms is significantly higher than in English. This leads to data sparsity issues where many valid word forms may not appear in the training set.
      \item \textbf{Ambiguity without accentuation:} Typically, stress marks are omitted in written Lithuanian. However, stress position and tone (acute, circumflex, or short) determine the meaning of monographic words. For instance, the word \textit{antis} can mean ``a duck'' or ``a bosom'' depending on the intonation/stress pattern: \textit{ántis} vs. \textit{añtis}. A grapheme-based model without accentuation marks may struggle to predict the correct prosody, resulting in monotonic or mispronounced speech.
\end{itemize}

To overcome these challenges, tools like \textbf{Kirčiuoklis} (Vytautas Magnus
University) are often employed in the text normalization pipeline. Kirčiuoklis
automatically assigns stress marks to raw text. One weakness of Kirčiuoklis is
that it relies on a word-dictionary based approach, which does not take into
account the context of the word. Thus, it suggests multiple possible
accentuation variants for ambiguous words, leaving it up to the user to select
the correct one.

In the absence of a high-quality, context-aware Grapheme-to-Phoneme (G2P)
converter for Lithuanian, this thesis will focus on grapheme-based TTS
synthesis with accentuation marks provided by Kirčiuoklis. In cases where
Kirčiuoklis suggests multiple accentuation variants for a word, no stress marks
will be added, leaving the TTS model to infer the correct prosody from context.

\subsection{Embeddings and Representation Learning}

\subsubsection{The Concept of Embeddings}

In machine learning, embeddings are dense vector representations of discrete
entities (such as words, characters, or speakers) to a high-dimensional
continuous vector space. Unlike one-hot encodings, which are sparse and highly
dimensional, embeddings provide a dense, lower-dimensional representation that
captures semantic relationships between underlying entities. For instance, in
word embeddings, similar words tend to have more similar (correlated) vector
representations, while dissimilar words map to more distant points in the
vector space.

\subsubsection{Text Embeddings}

The ``Encoder'' part of a TTS model is responsible for converting a sequence of
input symbols (characters or phonemes) into a sequence of feature vectors.
Typically, this is done using an embedding layer, which maps each input symbol
to a learnable fixed-size vector representation. During training, these
embeddings are learned jointly with the rest of the TTS model.

\subsubsection{Speaker Embeddings}

In order to enable multi-speaker synthesis, TTS models need to receive a
representation of the speaker's identity.

\textbf{Lookup Tables (LUT):} Early approaches used simple learnable embeddings where each speaker ID is
mapped to a unique vector. The vectors are learned joinly with the TTS model
during training. While efficient, this approach cannot generalize to speakers
not seen during training.

\textbf{Reference Encoders and Speaker Embeddings:}

Transfer learning approaches~\cite{jia2018transfer} have demonstrated adapting
speaker verification models to multispeaker TTS synthesis, enabling better
speaker adaptation and higher voice quality. A speaker encoder model
pre-trained on a massive, noisy dataset with thousands of speakers (such as
VoxCeleb) learns the general speaker space. Its pre-trained weights are then
frozen and used to extract embeddings for the TTS training data, allowing the
TTS model to effectively account for multi-speaker characteristics.

\textbf{d-vectors:} d-vectors~\cite{variani2014deep} are fixed-length speaker embeddings derived from a separate speaker verification model. A reference encoder network takes a reference audio recording of arbitrary length and compresses it into a fixed-length vector known as a d-vector, that summarizes the speaker's timbral and prosodic characteristics. These d-vectors are then provided as additional input to the TTS model, and are kept fixed during TTS training.

\textbf{x-vectors:} An evolution of d-vectors, x-vectors~\cite{snyder2018x} use a more Time Delay Neural Network (TDNN) architecture to capture the temporal context more effectively. These embeddings have shown an improved ability in zero-shot TTS scenarios.

One limitation of d-vectors and x-vectors is that if the reference audio is of
poor quality or contains background noise, the resulting speaker embedding may
not accurately represent the speaker's identity, leading to degraded synthesis
quality.

% TODO How Coqui TTS handles speakers: Specifically, how speaker embeddings are
% concatenated or added to the encoder outputs to condition the synthesis on a
% specific voice identity.

% TODO In Coqui TTS, speaker embeddings (either learned or computed via a reference
% encoder) are typically projected to the dimension of the encoder output and
% concatenated with the linguistic features before being passed to the decoder or
% variance adaptor.

\subsection{Deep learning for TTS}

The limitations of these complex, multi-stage pipelines motivated the creation
of a new approach, the end-to-end (E2E) model. E2E systems learn the entire
speech synthesis process --- from input text directly to acoustic output ---
using a single neural network. This approach promised to eliminate the need for
hand-crafted pipelines that were difficult to design, required extensive
expertise, and suffered from errors that accumulated across multiple
components. By learning directly from text-audio pairs, E2E models showed they
could produce speech with higher naturalness and expressiveness than previous
methods, representing a significant leap in TTS technology.

However, although deep learning TTS models are more robust to variations in
data quality compared to concatenative approaches, they are essentially
``data-hungry'' beasts that require large amounts of training data to achieve
optimal performance. This relationship between model performance and training
data size follows well-established scaling laws for neural
models~\cite{kaplan2020scaling}. This characteristic makes data selection and
quality control critical factors in developing effective neural TTS systems.

\subsubsection{Feedforward neural networks}

Feedforward Neural Networks (FNNs) are the simplest type of artificial neural
networks, consisting of layers of interconnected nodes (neurons) where
information flows in one direction --- from the input, through hidden layers,
to the output. While FNNs can be useful for basic regression or classification
tasks, they lack the memory and context-awareness needed for processing
sequential data like text and speech. Therefore, FNNs are not suitable for
modelling TTS tasks that require understanding of temporal dependencies.

\subsubsection{Encoder-Decoder architectures}

The Encoder-Decoder architecture is a neural network architecture consisting of
two components, namely an encoder and a decoder. The encoder processes the
input data and compresses it into a high-dimensional latent representation.
This vector captures the meaningful features of the input. The decoder uses
this latent representation as context to generate the final output. This
architecture is commonly used in sequence-to-sequence tasks, such as machine
translation (text-to-text) and text-to-speech synthesis (text-to-audio frames).

\subsubsection{Sequence-to-sequence models}

A common approach for TTS modelling it the sequence-to-sequence (seq2seq)
framework, which uses an encoder-decoder architecture with an attention
mechanism to map input (text) sequences to output (audio) sequences.

This architecture has three main components:

\textbf{Encoder:} The encoder processes through the input text sequence (graphemes or phonemes) and converts it into a high-level representation. Typically, this is done using recurrent neural networks (RNNs) or more complex modules like the CBHG (Convolution Bank + Highway network + Gated Recurrent Unit).
\textbf{Attention Mechanism:} The attention mechanism learns to align the high-level text representation with the output audio frames. At each decoding step, it determines which part (positions) of the input text the model should focus (or ``attend to'') in order to generate the next audio frame.
\textbf{Decoder:} Finally, the decoder --- also typically an RNN --- takes the encoder output, uses the attention output as context, and generates an acoustic representation of speech (typically mel-spectrogram) frame by frame. Being autoregressive, the decoder is a ``bottleneck'' during inference, as it must generate each frame sequentially, only after the previous one has been produced.

Tacotron~\cite{wang2017tacotron} and Tacotron~2~\cite{shen2018natural} are two
notable TTS models based on the sequence-to-sequence architecture. The variant
that this thesis primarily focuses on is Tacotron~2 with Double Decoder
Consistency (DDC).

In Tacotron~2, the encoder consists of a character embedding layer, followed by
a stack of convolutional layers (which encode local context about neighbouring
characters), and a bidirectional LSTM. The attention mechanism is
location-sensitive, computing an alignment between the encoder outputs (text)
and the decoder steps (audio frames). The decoder is an autoregressive RNN,
typically composed of LSTM layers.

\subsubsection{Non-autoregressive (parallel) models}

Unlike Tacotron~2, FastPitch~\cite{lancucki2020fastpitch} replaces the
recurrent layers with Transformer architecture blocks relying on
self-attention. Being a non-autoregressive model, FastPitch model can process
the entire input sequence in parallel, significantly speeding up both training
and inference.

Unlike Tacotron~2, which learns alignment implicitly using attention, FastPitch
requires an external aligner (or an internal alignment module) in order to
determine how many sound frames correspond to each input character. A duration
predictor network learns to predict these durations, ensuring the speech rhythm
is stable.

FastPitch includes a dedicated predictor that estimates the fundamental
frequency (F0) for every input symbol. The predicted pitch values are projected
and added to the hidden states, allowing the model to explicitly control
intonation before generating the spectrogram.

The primary advantages are inference speed (duet to non-autoregressive
generation), robustness (no attention failures like skipping or repeating
words), and controllability (pitch and speed can be tweaked manually during
synthesis).

\subsubsection{Notable End-to-End TTS models}

Besides Tacotron~2 and FastPitch, other notable TTS architectures include
\textbf{Glow-TTS}, which uses flow-based generative models for parallel
inference, and \textbf{VITS} (Conditional Variational Autoencoder with
Adversarial Learning), which combines the acoustic TTS model (Glow-TTS) with a
neural vocoder (HiFi-GAN) into a single end-to-end architecture.

\subsubsection{Neural vocoders}

Standard TTS models like Tacotron~2 or FastPitch generate intermediate acoustic
features (typically mel-spectrograms), but do not directly produce raw audio
waveforms. However, mel-spectrograms are lossy representations that only
capture the magnitude of the frequency bands, discarding phase information.
This information is discarded during the STFT process. Thus, converting a
spectrogram into audio is a non-trivial task, as the phase information must be
estimated. This challenge is known as the \textit{inversion problem}.

The tool used to convert spectrograms back into waveforms is called a vocoder.

Traditionally, the Griffin-Lim algorithm~\cite{griffin1984signal} has been used
to iteratively estimate and reconstruct the phase information from the
magnitude spectrogram. However, this method often produces audio with
noticeable artifacts and lower quality compared to natural speech.

Neural GAN (Generative Adversarial Network) based vocoders, such as
HiFi-GAN~\cite{kong2020hifi}, have become the dominant approach for vocoding in
modern TTS systems. These GAN-based vocoder models consist of two main
components: a Generator, which reconstructs mel-spectrograms into raw
waveforms, and a Discriminator, which distinguishes between real and
synthesized audio. This adversarial training forces the Generator to produce
high-fidelity audio that closely resembles natural speech.

Coqui TTS comes with a pre-trained HiFi-GAN v2 vocoder trained on a large
multi-speaker dataset (VCTK). Given its widespread adoption and status as a
state-of-the-art neural vocoder, this thesis will use the HiFi-GAN v2 model for
waveform generation. In order to ensure compatibility between the TTS models
and the vocoder, the TTS models' acoustic parameters will be configured to the
exact same settings (sampling rate, FFT parameters, Mel filterbank settings) as
those used during the HiFi-GAN v2 training.

\subsection{Multi-speaker TTS}

Multi-speaker TTS models are designed to synthesize speech in the voices of
multiple speakers. As the name suggests, these models are trained on data from
many different speakers, allowing them to learn the characteristics of each
voice and synthesize speech that sounds like a specific individual, while still
being able to generalize the shared linguistic and acoustic patterns across
speakers.

\subsubsection{Techniques}

Techniques for multi-speaker TTS often involve conditioning the model on
speaker identity. This can be done by providing a speaker embedding vector as
an additional input to the model. Early successful implementations of this
approach include Deep Voice 2~\cite{gibiansky2017deep}, which demonstrated
effective multi-speaker synthesis by learning speaker-specific embeddings.

The speaker embedding can be learned jointly with the TTS model during
training, or it can be pre-trained using a separate speaker verification model.

\subsubsection{Challenges}

One key challenge in multi-speaker TTS is ensuring that the model can
effectively capture the unique characteristics of each speaker's voice while
still producing natural-sounding speech.

The distribution of training data across speakers can also influence the
model's performance and its ability to mimic all voices equally well. The
training datasets are often strongly imbalanced, with some speakers having
hours of annotated speech, while others may have only a few minutes. This
imbalance can cause the model to overfit to the speakers with more data,
resulting in lower synthesis quality for underrepresented speakers.

Thus, data selection and sampling strategies are critical considerations when
training multi-speaker TTS models.

\subsection{Data selection in TTS}

As TTS models require large amounts of annotated speech data for training,
training on all available data can be computationally prohibitive and
time-consuming. Therefore, data selection aims to identify optimal subsets of
the available data that can yield high-quality synthesis while minimizing
training time and resource requirements.

\subsubsection{Data quantity}

The relationship between the quantity of training data and model performance in
neural networks follows predictable scaling laws. Research by Kaplan et
al.~\cite{kaplan2020scaling} showed that model performance generally improves
as a power law function of the amount of training data, with diminishing
returns at larger scales. Similar trends have been observed in TTS models,
where increasing the training data size leads to better synthesis quality and
naturalness, but with diminishing improvements as the dataset grows larger.

While this would suggest that ``more data is better'', in practice, collecting,
curating, and processing large-scale multi-speaker TTS training datasets, as
well as the computational burden of training on them, can be prohibitive. This
makes efficient data selection strategies increasingly valuable for achieving
optimal performance-to-cost ratios.

\subsubsection{Data selection strategies}

% TODO

% In the context of multi-speaker TTS, this involves selecting a diverse set of
% speakers and utterances that maximize acoustic variety while minimizing
% redundancy.

% Given the scaling laws of deep learning~\cite{kaplan2020scaling}, simply
% increasing dataset size does not always yield proportional improvements,
% especially if the data is noisy or redundant. Data selection strategies aim to
% optimize the training set $S \subset D$ such that a model trained on $S$
% achieves performance comparable to or better than one trained on $D$.

\subsection{Evaluation methods for TTS}

\subsubsection{Subjective evaluation}

TTS systems are often evaluated using subjective listening tests, where human
listeners rate the naturalness and intelligibility of synthesized speech
samples.

A common subjective evaluation method is the Mean Opinion Score~\cite{itup800}
(MOS) test. In a MOS test, listeners are presented with a set of synthesized
speech samples and are asked to rate each sample on a scale from 1 to 5. Here,
the scale typically represents the following levels of quality:

\begin{itemize}
      \item 1 - Bad: The speech is completely unnatural and unintelligible.
      \item 2 - Poor: The speech is mostly unnatural and difficult to understand.
      \item 3 - Fair: The speech is somewhat natural but has noticeable artifacts or
            issues.
      \item 4 - Good: The speech is mostly natural with minor artifacts that do not
            significantly affect intelligibility.
      \item 5 - Excellent: The speech is indistinguishable from natural human speech.
\end{itemize}

\subsubsection{Objective evaluation}

Objective evaluation methods use computational metrics to assess the quality of
synthesized speech.

Common objective metrics include:

\begin{itemize}
      \item Mel-Cepstral Distortion~\cite{kubichek1993mel} (MCD): Measures the spectral
            distance between synthesized and natural speech.
      \item Fundamental Frequency Root Mean Square Error (F0 RMSE): Assesses the accuracy
            of pitch contours.
\end{itemize}

\subsection{Summary}

Modern TTS synthesis has evolved from complex, multi-stage concatenative and
statistical pipelines to end-to-end deep learning architectures. Models such as
Tacotron~2 and FastPitch, combined with neural vocoders like HiFi-GAN, are
capable of generating natural-sounding speech that is often indistinguishable
from human speech. These systems rely on digital signal processing techniques
to convert raw audio into representations like Mel-spectrograms --- and learned
embeddings to handle text-to-audio alignment and speaker identity.

However, these deep learning models are notoriously data-hungry, requiring
large amounts of high-quality, annotated speech. While English benefits from
massive, well-curated datasets (e.g., LJSpeech, VCTK), low-resource languages
face distinct challenges. Specifically, Lithuanian possesses a complex prosodic
structure characterized by free stress and pitch accents, requiring accurate
text normalization and accentuation handling. Furthermore, multi-speaker
synthesis requires the model not only to learn these linguistic features but
also to generalize across a latent speaker space, typically managed via
d-vector or x-vector embeddings.

\subsection{Research Gap}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Methodology
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

\subsection{Research design}

The independent variables in this study are:

\begin{itemize}
      \item Data selection strategy: Different methods for selecting subsets of the
            training data.
      \item TTS model architecture: Comparing different TTS architectures.
\end{itemize}

The dependent variables in this study are:

\begin{itemize}
      \item TTS model performance: Measured using objective metrics (MCD, F0 RMSE) and
            subjective evaluations (MOS scores).
\end{itemize}

The controlled variables in this study are:

\begin{itemize}
      \item Dataset: The Liepa~2 Lithuanian speech corpus.
      \item Training data size: The same total amount of training data used across all
            experiments.
      \item Training procedure: The same training hyperparameters and protocols applied
            across all experiments.
      \item Evaluation metrics: The same objective and subjective evaluation methods
            applied across all experiments.
\end{itemize}

\subsection{Data and preprocessing}

\subsubsection{Liepa~2 dataset}

The primary dataset for this research is the Liepa~2 corpus, a comprehensive
collection of Lithuanian speech data. The corpus encompasses 1000~hours of
short voice recordings from 2621~speakers, along with corresponding text
transcriptions. The recordings span various speech styles and contexts,
including read speech (audiobooks, news), TV and radio broadcasts, spontaneous
speech.

More specifically, the dataset contains:
\begin{itemize}
      \item Total duration: 1000~hours of speech data.
      \item Number of speakers: 2621~unique speakers.
      \item Number of utterances: 1,874,648~recorded utterances.
\end{itemize}

\subsubsection{Acoustic preprocessing}

The raw audio recordings from the Liepa~2 dataset are sampled at 16~kHz. To
prepare the audio for TTS model training, the audio waveforms are resampled to
22,050~Hz. While resampling to a higher frequency does not add new information,
it will be compatible with pre-trained vocoders that expect 22,050~Hz input.

Additional preprocessing steps, such as silence trimming and normalization, are
performed by the Coqui TTS framework during training.

The acoustic processing that transforms audio waveforms into mel-spectrograms
is performed on-the-fly during model training by the Coqui TTS framework. The
mel-spectrogram parameters used are as follows:

\begin{itemize}
      \item fft\_size: 1024
      \item win\_length: 1024
      \item hop\_length: 256
      \item frame\_length\_ms: null
      \item frame\_shift\_ms: null
      \item stft\_pad\_mode: "reflect"
      \item sample\_rate: 22050
      \item resample: false
      \item preemphasis: 0.98
      \item ref\_level\_db: 20
      \item do\_sound\_norm: false
      \item log\_func: "np.log10"
      \item do\_trim\_silence: true
      \item trim\_db: 60
      \item do\_rms\_norm: false
      \item db\_level: null
      \item power: 1.5
      \item griffin\_lim\_iters: 60
      \item num\_mels: 80
      \item mel\_fmin: 0.0
      \item mel\_fmax: 8000.0
      \item spec\_gain: 20
      \item do\_amp\_to\_db\_linear: true
      \item do\_amp\_to\_db\_mel: true
      \item pitch\_fmax: 640.0
      \item pitch\_fmin: 1.0
      \item signal\_norm: true
      \item min\_level\_db: -100
      \item symmetric\_norm: true
      \item max\_norm: 4.0
      \item clip\_norm: true
      \item stats\_path: null
\end{itemize}

\subsubsection{Text normalization}

Before feeding text data into TTS models, the text undergoes normalization to
convert it into a more consistent and model-friendly format. The Liepa~2 text
already includes a significant level of normalization - for instance, the
following elements are written exactly as they were spoken: dates, times,
acronyms, abbreviations, numbers.

However, some additional normalization steps are applied to further standardize
the text:

\begin{itemize}
      \item Punctuation standardization: Replacing uncommon punctuation marks with more
            common equivalents (e.g., replacing em dashes with hyphens, and semicolons with
            commas).
      \item Removal of extraneous characters: Eliminating any remaining characters that are
            not letters, digits, whitespace, or basic punctuation (.,-?!).
      \item Whitespace normalization: Collapsing multiple consecutive whitespace characters
            into a single space and trimming leading/trailing whitespace.
      \item Accent addition: Adding accent marks (tilde, acute, grave) to words using
            Kirčiuoklis tool (where Kirčiuoklis suggests multiple options, accent is not
            added).
      \item Lowercasing: Converting all text to lowercase to reduce vocabulary size.
      \item Letter replacements: Substituting non-Lithuanian letters with their Lithuanian
            equivalents (`w' with `v', `q' with `kv', and `x' with `ks').
\end{itemize}

In order to add the accents to the text, the online tool Kirčiuoklis was
queried with all unique words from the dataset. The tool returned the accented
versions of the words, which were then used to replace the unaccented words in
the text. In cases where Kirčiuoklis provided multiple accentuation options for
a word, no accent was added to avoid introducing errors.

As a result of these normalization steps, the vocabulary size is reduced from
140~characters to 41~characters, simplifying the learning task for TTS models.

\subsection{Data subset creation}

To investigate the impact of data selection on TTS performance, several
strategies for selecting subsets of the Liepa~2 dataset were employed:

\begin{itemize}
      \item Random sampling: Randomly selecting a subset of the training data.
      \item Speaker diversity maximization: Selecting samples to maximize the number of
            unique speakers in the subset.
            % \item Phonetic coverage optimization: Choosing samples to cover the widest range of
            %       phonetic contexts.
            % \item Quality-based selection: Selecting samples based on audio quality metrics,
            %       prioritizing clearer recordings.
\end{itemize}

\subsection{Model training}

\subsubsection{Tacotron~2 with DDC}

The Tacotron~2 model was configured with the following hyperparameters:

\begin{table}[h!]
\end{table}

\subsubsection{FastPitch}

The FastPitch model was configured with the following hyperparameters:

\begin{table}[h!]
\end{table}

\subsubsection{Computational hardware}

The experiments were conducted on a personal high-performance computing setup
with the following specifications:

\begin{itemize}
      \item CPU: AMD Epyc 7642 48-Core, 96~thread processor
      \item RAM: 256 GB DDR4 3200 MHz
      \item GPU: NVIDIA GeForce RTX 3090 with 24 GB VRAM
      \item Storage: 2 TB NVMe SSD
\end{itemize}

\subsection{Evaluation protocol}

\subsubsection{Test set}

Evaluation was performed on a held-out test set of 100~phrases from the Liepa~2
dataset.

\subsubsection{Objective evaluation}

\subsubsection{Subjective evaluation}

Subjective evaluation was conducted using Mean Opinion Score (MOS) tests.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Results and analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and analysis}

\subsection{Tacotron~2 with DDC}

\subsubsection{Objective results}

\subsubsection{Subjective results}

\subsubsection{Qualitative analysis}

\subsection{FastPitch}

\subsubsection{Objective results}

\subsubsection{Subjective results}

\subsubsection{Qualitative analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

\subsection{Summary of findings}

\subsection{Contributions}

\subsection{Limitations of the study}

\subsection{Future work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\phantom{Appendix} References}

\printbibliography[heading=none]

% Examples are also provided for ChatGPT citation, both in general \cite{chatgpt_bendrai} and for a specific conversation \cite{chatgpt_pokalbis}.

\end{document}
