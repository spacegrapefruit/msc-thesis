%%%%%
%%%%%  Use LUALATEX, not LATEX.
%%%%%
%%%%
\documentclass[]{VUMIFTemplateClass}

\usepackage{indentfirst}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{mathtools}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage[hidelinks]{hyperref}
\usepackage{color,algorithm,algorithmic}
\usepackage[nottoc]{tocbibind}
\usepackage{tocloft}

\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage}

\makeatletter
\renewcommand{\fnum@algorithm}{\thealgorithm}
\makeatother
\renewcommand\thealgorithm{\arabic{algorithm} algorithm}

\usepackage{biblatex}
\bibliography{bibliografija}
%% to change the numbering (numeric or alphabetic) of bibliographic sources, make the change in VUMIFTemplateClass.cls, line 139

% Author's MACROS
\newcommand{\EE}{\mathbb{E}\,} % Mean
\newcommand{\ee}{{\mathrm e}}  % nice exponent
\newcommand{\RR}{\mathbb{R}}

\studyprogramme{Data Science}
\worktype{Master's thesis}
\worktitle{Data Selection Strategies for Multi-Speaker Text-to-Speech Synthesis in Lithuanian}
\secondworktitle{Work Title in Lithuanian}
\workauthor{Aleksandr Jan Smoliakov}

\supervisor{Gerda Ana Melnik-Leroy}
\reviewer{pedagogical/scientific title Name Surname}
%If present, otherwise delete
\scientificadvisor{pedagogical/scientific title Name Surname}
%If present, otherwise delete

\begin{document}
\selectlanguage{english}

\onehalfspacing
\input{TitlePage}

% %% TODO Acknowledgements Section
% \sectionnonumnocontent{Acknowledgements}
% The author is thankful the Information Technology Research Center, Faculty of Mathematics and Informatics, Vilnius University, for providing the High-Performance Computing (HPC) resources for this research.
% %%
% %%
% %%      If you have used IT resources (CPU-h, GPU-h, other IT resources) provided by MIF for your thesis research, please leave the acknowledgement; if you have not, you can delete it.
% %%
% %%

% You can also add here acknowledgements for various other things, such as your supervisor, university, company, etc.

\singlespacing
\selectlanguage{english}

% list of figures, delete if not needed
\listoffigures

% list of tables, delete if not needed
\listoftables

% Table of contents
\tableofcontents
\onehalfspacing

\sectionnonum{Introduction}

The goal of creating machines that can speak like humans has captivated
researchers for centuries. One of the earliest known attempts dates back to the
18th century, with Wolfgang von Kempelen's mechanical speech machine that
utilized a bellows-driven lung and physical models of the tongue and lips.

Over the centuries, advancements in technology and understanding of human
speech have driven significant progress in this field. Today's state-of-the-art
systems, dominated by end-to-end (E2E) neural models, have achieved highly
naturalistic speech with unprecedented acoustic quality. Notably, these
end-to-end systems have unified the entire synthesis process into a single
neural network, eliminating the need for complex multi-stage pipelines.

\subsection{Background and motivation}

\subsection{Problem statement}

Training high-quality TTS models typically requires large amounts of annotated
speech data. The common recommendation is to use at least 10~hours of recorded
speech from a single speaker to achieve good results.

Liepa~2 is a recently released Lithuanian speech corpus that contains
1000~hours of annotated speech; however, this data is distributed across more
than 2600~speakers, with most speakers contributing only a few minutes of
speech. The top speaker has around 2.5~hours of recorded speech.

Training a high-quality single-speaker TTS model on such limited data poses a
challenge. Multi-speaker TTS models can utilize data from multiple speakers to
improve performance. However, training on all available data is a
time-consuming and computationally expensive process, especially in the context
of a master's thesis.

Therefore, it makes sense to explore strategies for selecting smaller subsets
of the available data for training. The question that arises is, what is the
best way to sample multi-speaker data for training TTS models?

\subsection{Research questions}

This thesis aims to answer the following research questions:

\begin{itemize}
      \item TODO
\end{itemize}

\subsection{Objectives}

\subsection{Scope of the study}

Scope: This study is exclusively focused on the Lithuanian language and the
Liepa~2 speech corpus. It investigates a fixed total training data size of
30~hours. The models are limited to Tacotron~2 with DDC and FastPitch
architectures within the Coqui TTS framework, using a pre-trained WaveGlow
vocoder for waveform generation.

Limitations: The findings may not generalize to other languages, datasets with
different characteristics, or other TTS architectures. The 30-hour training
data size is a practical constraint and may not reflect performance at larger
scales.

\subsection{Thesis structure}

\section{Literature review}

Before diving into the specifics of text normalization, it is important to
establish the broader context of audio processing, text-to-speech synthesis,
and its evolution over time.

\subsection{Digital signal processing fundamentals}

\subsubsection{Physical properties of sound}

In the physical world, sound is a continuous pressure wave that propagates
through a medium, such as air.

Key properties of sound waves include frequency (pitch), amplitude (loudness),
and phase.

The human audible range is typically considered to be from 20~Hz to 20~kHz,
with most speech information concentrated between 100~Hz and 8~kHz.

\subsubsection{Digital representation of audio}

As the real-world sound wave is continuous, it must be converted into a digital
format for processing by computers. This involves two main steps, namely
sampling and quantization.

Sampling is the process of measuring the amplitude of the sound wave at regular
time intervals.

The rate at which these samples are taken is called the sampling rate (or
sample frequency), measured in Hertz (Hz). According to the Nyquist-Shannon
sampling theorem, accurate reconstruction of a continuous signal requires a
sampling rate that is at least twice the highest frequency present in the
signal. In text-to-speech applications , common sampling rates for audio are
22.05~kHz and 24~kHz, which can capture frequencies up to approx. 11~kHz and
12~kHz, respectively.

Quantization (also known as bit depth) is the mapping of continuous amplitude
values to discrete levels for digital representation. Typical bit depths for
audio are 16-bit and 24-bit PCM (Pulse Code Modulation) formats, which provide
65,536 and 16,777,216 discrete amplitude levels, respectively.

This process introduces a small amount of quantization noise, which can affect
audio quality if the bit depth is too coarse.

% Pre-emphasis: High-frequency filtering to balance the frequency spectrum before
% processing.

\subsection{Time-Frequency Analysis}

\subsubsection{Fourier Transform}

Fourier Transform (FT) is a mathematical technique that transforms a
time-domain signal (such as an audio waveform) into its frequency-domain
representation. The signal is decomposed into a sum of sine and cosine waves at
various frequencies, each with a specific amplitude and phase. This allows us
to analyze the frequency content of the signal.

Short-Time Fourier Transform (STFT) extends the FT by applying it to short,
overlapping segments (frames) of the signal. This transformation provides a
time-frequency representation, showing how the frequency content of the signal
changes over time.

% Windowing functions: Hamming or Hann windows to reduce spectral leakage.

\subsubsection{Spectrogram}

The spectrogram is a visual representation of the STFT, displaying frequency on
the vertical axis, time on the horizontal axis, and amplitude

% Visual representation of the spectrum of frequencies of a signal as it varies
% with time.

% Linear vs. Logarithmic amplitude scales (Decibels).

\subsubsection{Mel scale and Mel-spectrograms}

% Psychoacoustics: How human hearing is non-linear (we perceive pitch differences
% better at lower frequencies).

% The Mel Filterbank: Mapping the linear frequency scale to the Mel scale to
% mimic human hearing.

% Why Mel-Spectrograms are the standard input feature for modern TTS systems
% (compressed representation of audio).

\subsection{Text-to-speech synthesis}

Text-to-Speech (TTS) synthesis, also known as speech synthesis, is the process
of converting written text into human-like spoken words. Nowadays TTS is a key
technology in numerous applications, including virtual assistants,
accessibility tools, and language learning platforms.

\subsubsection{Traditional TTS approaches}

The first known attempts to create artificial speech date back to the late 18th
century. These early devices were purely mechanical, relying on physical models
to mimic the human vocal tract. One notable example is Wolfgang von Kempelen's
invention called the ``Acoustic-Mechanical Speech Machine'', which utilized a
bellows-driven lung and physical models of the tongue and lips to articulate
both vowels and consonants. Such early efforts had limited success, being able
to produce only individual phonemes or simple syllables.

The advancement of electricity and electronics in the late 19th and early 20th
centuries opened new possibilities for speech synthesis. Homer Dudley's Voder
(Voice Operation Demonstrator), unveiled in 1937, represented the first
successful electronic recreation of human speech. This invention allowed a
trained human operator to synthesize intelligible speech by manipulating a
complex arrangement of keyboards and pedals, each controlling different
acoustic parameters of speech production. The Voder marked a shift from
mechanical to electronic synthesis methods.

In the decades that followed, two main approaches for speech synthesis emerged:
concatenative synthesis and parametric synthesis.

\subsubsection{Concatenative synthesis}

The concatenative synthesis approach synthesizes speech by piecing together
pre-recorded segments of human speech. This method involves several steps.
First, it requires pre-recording a large database of speech segments spoken by
a human voice actor. Each segment is labeled and indexed based on its phonetic
and prosodic properties.

During synthesis, the system breaks down the input text into short linguistic
units (such as phonemes or syllables) using a text analysis module. Then, it
queries the speech database to find the best-matching segments for each unit.
The retrieved segments are blended and concatenated to form a continuous speech
waveform. The system then uses signal processing techniques to smooth the
transitions between segments and adjust pitch and duration to match the desired
output characteristics.

Concatenative synthesis can produce natural-sounding individual speech units,
but the final audio often has noticeable glitches and breaks at the points
where segments are joined together. The segments may not blend smoothly due to
differences in pitch, duration, and timbre. The prosody also tends to sound
``choppy'' and unnatural, since stringing disjointed segments together does not
capture the natural rhythm and intonation patterns of connected speech.5

Finally, concatenative synthesis requires language-specific expertise to design
and maintain the underlying speech database and selection algorithms. This need
for extensive data can make it challenging to develop concatenative TTS systems
for low-resource languages or dialects.

\subsubsection{Parametric synthesis}

In contrast, statistical parametric speech synthesis (SPSS) uses statistical
models, typically Hidden Markov Models (HMMs), to generate the parameters that
control a speech waveform.

This method involves training a statistical model on a large corpus of recorded
speech. The model learns the relationship between linguistic features (like
phonemes and prosody) and the acoustic features of the speech signal, such as
spectral envelope and fundamental frequency. During synthesis, the system takes
text as input, converts it to a sequence of linguistic features, and then uses
the trained model to generate a corresponding sequence of acoustic parameters.

Compared to concatenative synthesis, the statistical approach allows for more
flexibility and control over the speech synthesis process, enabling the
generation of a wider variety of voices and speaking styles. However, HMM-based
synthesis had a persistent problem: the statistical averaging built into the
models tended to over-smooth the acoustic features, creating a characteristic
``buzzy'' or ``muffled'' sound that lacked the sharpness and detail of natural
human speech.2

\subsection{Linguistic Representation (Text Processing)}

2.3.1. Text Normalization:

Converting non-standard words (numbers, dates, abbreviations) into spoken
format (e.g., "1990" -> "tūkstantis devyni šimtai devyniasdešimt").

Specific challenges in Lithuanian (inflection, accentuation).

2.3.2. Graphemes vs. Phonemes:

Graphemes: Using raw characters as input.

Phonemes: Using phonetic transcription (IPA or X-SAMPA).

Grapheme-to-Phoneme (G2P): The process of converting written text to phonetic
representation.

\subsection{Embeddings and Representation Learning}

2.4.1. The Concept of Embeddings:

Definition: Mapping discrete variables (words, characters, speaker IDs) to
continuous vectors in a high-dimensional space.

2.4.2. Text Embeddings:

How the "Encoder" part of a TTS model converts a sequence of
characters/phonemes into a sequence of hidden feature vectors.

2.4.3. Speaker Embeddings (Crucial for Multi-Speaker TTS):

Lookup Tables (LUT): Simple learnable embeddings for each speaker ID. (Why this
fails with new/unseen speakers).

d-vectors and x-vectors: Fixed-length vectors derived from a separate speaker
verification model.

Global Style Tokens (GST): Unsupervised learning of style and prosody.

How Coqui TTS handles speakers: Specifically, how speaker embeddings are
concatenated or added to the encoder outputs to condition the synthesis on a
specific voice identity.

\subsection{Deep learning for TTS}

The limitations of these complex, multi-stage pipelines opened the door for a
new approach, the end-to-end (E2E) model. E2E systems learn the entire speech
synthesis process--from input text directly to acoustic output--using a single
neural network.8 This approach promised to eliminate the need for hand-crafted
pipelines that were difficult to design, required extensive expertise, and
suffered from errors that accumulated across multiple components.10 By learning
directly from text-audio pairs, E2E models showed they could produce speech
with higher naturalness and expressiveness than previous methods, representing
a significant leap in TTS technology.9

\subsubsection{Sequence-to-sequence models}

% 2.5.1. Sequence-to-Sequence (Seq2Seq) Models:

% Tacotron 2:

% Encoder (Character embeddings, Convolutional layers).

% Decoder (LSTM-based, Autoregressive generation of Mel-spectrogram frames).

% Attention Mechanism (Location-sensitive attention) – alignment between text and
% audio.

The main architecture for modern E2E TTS is the sequence-to-sequence (seq2seq)
framework, known through e.g. Google's Tacotron model.10 This framework has
three main parts: an encoder, an attention mechanism, and a decoder.8 The
encoder--usually a recurrent neural network (RNN) or a more complex module like
the CBHG (Convolution Bank + Highway network + Gated Recurrent Unit)--processes
the input text and converts it into a high-level representation.11 The
attention mechanism acts as a bridge, learning to align the text representation
with the output audio frames. At each step, it decides which part of the input
text the model should focus on to generate the next piece of audio.8 The
decoder, also typically an RNN, takes the encoder output and uses the attention
context to generate an acoustic representation of the speech, one frame at a
time.8

\subsubsection{Non-autoregressive models}

% 2.5.2. Non-Autoregressive (Parallel) Models:

% FastPitch:

% Feed-forward Transformer blocks (Self-attention).

% Explicit Duration Predictor (predicting how long each phoneme lasts).

% Pitch prediction: Predicting fundamental frequency (F0) contours.

% Advantages: Faster inference, better control over prosody.

\subsubsection{Neural vocoders}

% 2.6.1. The Inversion Problem:

% Why we cannot simply "invert" a Mel-spectrogram back to audio (loss of phase
% information).

% The Griffin-Lim algorithm (traditional method) vs. Neural Vocoders.

% 2.6.2. Generative Adversarial Networks (GANs) in Audio:

% HiFi-GAN:

% Generator: Upsampling Mel-spectrograms to raw waveforms.

% Discriminator: Distinguishing between real and synthesized audio to force high
% fidelity.

% Why HiFi-GAN v2 is the chosen vocoder for this thesis.

% ---

% An important design choice in models like Tacotron is that the decoder doesn't
% directly create the final audio waveform. Instead, it predicts an intermediate
% acoustic representation, typically a mel-spectrogram.6

TTS models like Tacotron or FastPitch typically do not directly produce raw
audio waveforms. Instead, they generate intermediate acoustic features (such as
mel-spectrograms) rather than raw audio waveforms. Converting these features
into an audio signal requires a separate component called a vocoder.

Traditional vocoders, like Griffin-Lim, use signal processing techniques to
reconstruct waveforms from spectrograms. However, these methods often produce
audio with noticeable artifacts and lower quality compared to natural speech.

Neural vocoders, such as WaveNet and WaveGlow, have revolutionized this step by

\subsubsection{Notable End-to-End TTS models}

\subsection{Multi-speaker TTS}

Multi-speaker TTS models are designed to synthesize speech in the voices of
multiple speakers. As the name suggests, these models are trained on data from
many different speakers, allowing them to learn the characteristics of each
voice and synthesize speech that sounds like a specific individual, while still
being able to generalize the shared linguistic and acoustic patterns across
speakers.

\subsubsection{Advantages}

\subsubsection{Techniques}

Techniques for multi-speaker TTS often involve conditioning the model on
speaker identity. This can be done by providing a speaker embedding vector as
an additional input to the model.

The speaker embedding can be learned jointly with the TTS model during
training, or it can be pre-trained using a separate speaker verification model.

\subsubsection{Challenges}

\subsection{Data selection in TTS}

\subsubsection{Data quantity}

\subsubsection{Data quality}

\subsubsection{Data selection strategies}

\subsection{Evaluation methods for TTS}

\subsubsection{Subjective evaluation}

TTS systems are often evaluated using subjective listening tests, where human
listeners rate the naturalness and intelligibility of synthesized speech
samples.

A common subjective evaluation method is the Mean Opinion Score (MOS) test. In
a MOS test, listeners are presented with a set of synthesized speech samples
and are asked to rate each sample on a scale from 1 to 5. Here, the scale
typically represents the following levels of quality:

\begin{itemize}
      \item 1 - Bad: The speech is completely unnatural and unintelligible.
      \item 2 - Poor: The speech is mostly unnatural and difficult to understand.
      \item 3 - Fair: The speech is somewhat natural but has noticeable artifacts or
            issues.
      \item 4 - Good: The speech is mostly natural with minor artifacts that do not
            significantly affect intelligibility.
      \item 5 - Excellent: The speech is indistinguishable from natural human speech.
\end{itemize}

\subsubsection{Objective evaluation}

Objective evaluation methods use computational metrics to assess the quality of
synthesized speech.

Common objective metrics include:

\begin{itemize}
      \item Mel-Cepstral Distortion (MCD): Measures the spectral distance between
            synthesized and natural speech.
      \item Fundamental Frequency Root Mean Square Error (F0 RMSE): Assesses the accuracy
            of pitch contours.
\end{itemize}

\subsection{Summary and research gap}

\section{Methodology}

\subsection{Research design}

The independent variables in this study are:

\begin{itemize}
      \item Data selection strategy: Different methods for selecting subsets of the
            training data.
      \item TTS model architecture: Comparing different TTS architectures.
\end{itemize}

The dependent variables in this study are:

\begin{itemize}
      \item TTS model performance: Measured using objective metrics (MCD, F0 RMSE) and
            subjective evaluations (MOS scores).
\end{itemize}

The controlled variables in this study are:

\begin{itemize}
      \item Dataset: The Liepa~2 Lithuanian speech corpus.
      \item Training data size: The same total amount of training data used across all
            experiments.
      \item Training procedure: The same training hyperparameters and protocols applied
            across all experiments.
      \item Evaluation metrics: The same objective and subjective evaluation methods
            applied across all experiments.
\end{itemize}

\subsection{Data and preprocessing}

\subsubsection{Liepa~2 dataset}

The primary dataset for this research is the Liepa~2 corpus, a comprehensive
collection of Lithuanian speech data. The corpus encompasses 1000~hours of
short voice recordings from 2621~speakers, along with corresponding text
transcriptions. The recordings span various speech styles and contexts,
including read speech (audiobooks, news), TV and radio broadcasts, spontaneous
speech.

More specifically, the dataset contains:
\begin{itemize}
      \item Total duration: 1000~hours of speech data.
      \item Number of speakers: 2621~unique speakers.
      \item Number of utterances: 1,874,648~recorded utterances.
\end{itemize}

\subsubsection{Acoustic preprocessing}

The raw audio recordings from the Liepa~2 dataset are sampled at 16~kHz. To
prepare the audio for TTS model training, the audio waveforms are resampled to
22,050~Hz. While resampling to a higher frequency does not add new information,
it will be compatible with pre-trained vocoders that expect 22,050~Hz input.

Additional preprocessing steps, such as silence trimming and normalization, are
performed by the Coqui TTS framework during training.

The acoustic processing that transforms audio waveforms into mel-spectrograms
is performed on-the-fly during model training by the Coqui TTS framework. The
mel-spectrogram parameters used are as follows:

\begin{itemize}
      \item fft\_size: 1024
      \item win\_length: 1024
      \item hop\_length: 256
      \item frame\_length\_ms: null
      \item frame\_shift\_ms: null
      \item stft\_pad\_mode: "reflect"
      \item sample\_rate: 22050
      \item resample: false
      \item preemphasis: 0.98
      \item ref\_level\_db: 20
      \item do\_sound\_norm: false
      \item log\_func: "np.log10"
      \item do\_trim\_silence: true
      \item trim\_db: 60
      \item do\_rms\_norm: false
      \item db\_level: null
      \item power: 1.5
      \item griffin\_lim\_iters: 60
      \item num\_mels: 80
      \item mel\_fmin: 0.0
      \item mel\_fmax: 8000.0
      \item spec\_gain: 20
      \item do\_amp\_to\_db\_linear: true
      \item do\_amp\_to\_db\_mel: true
      \item pitch\_fmax: 640.0
      \item pitch\_fmin: 1.0
      \item signal\_norm: true
      \item min\_level\_db: -100
      \item symmetric\_norm: true
      \item max\_norm: 4.0
      \item clip\_norm: true
      \item stats\_path: null
\end{itemize}

\subsubsection{Text normalization}

Before feeding text data into TTS models, the text undergoes normalization to
convert it into a more consistent and model-friendly format. The Liepa~2 text
already includes a significant level of normalization - for instance, the
following elements are written exactly as they were spoken: dates, times,
acronyms, abbreviations, numbers.

However, some additional normalization steps are applied to further standardize
the text:

\begin{itemize}
      \item Punctuation standardization: Replacing uncommon punctuation marks with more
            common equivalents (e.g., replacing em dashes with hyphens, and semicolons with
            commas).
      \item Removal of extraneous characters: Eliminating any remaining characters that are
            not letters, digits, whitespace, or basic punctuation (.,-?!).
      \item Whitespace normalization: Collapsing multiple consecutive whitespace characters
            into a single space and trimming leading/trailing whitespace.
      \item Accent addition: Adding accent marks (tilde, acute, grave) to words using
            Kirčiuoklis tool (where Kirčiuoklis suggests multiple options, accent is not
            added).
      \item Lowercasing: Converting all text to lowercase to reduce vocabulary size.
      \item Letter replacements: Substituting non-Lithuanian letters with their Lithuanian
            equivalents (`w' with `v', `q' with `kv', and `x' with `ks').
\end{itemize}

In order to add the accents to the text, the online tool Kirčiuoklis was
queried with all unique words from the dataset. The tool returned the accented
versions of the words, which were then used to replace the unaccented words in
the text. In cases where Kirčiuoklis provided multiple accentuation options for
a word, no accent was added to avoid introducing errors.

As a result of these normalization steps, the vocabulary size is reduced from
140~characters to 41~characters, simplifying the learning task for TTS models.

\subsection{Data subset creation}

To investigate the impact of data selection on TTS performance, several
strategies for selecting subsets of the Liepa~2 dataset were employed:

\begin{itemize}
      \item Random sampling: Randomly selecting a subset of the training data.
      \item Speaker diversity maximization: Selecting samples to maximize the number of
            unique speakers in the subset.
            % \item Phonetic coverage optimization: Choosing samples to cover the widest range of
            %       phonetic contexts.
            % \item Quality-based selection: Selecting samples based on audio quality metrics,
            %       prioritizing clearer recordings.
\end{itemize}

\subsection{Model training}

\subsubsection{Tacotron~2 with DDC}

The Tacotron~2 model was configured with the following hyperparameters:

\begin{table}[h!]
\end{table}

\subsubsection{FastPitch}

The FastPitch model was configured with the following hyperparameters:

\begin{table}[h!]
\end{table}

\subsubsection{Computational hardware}

The experiments were conducted on a personal high-performance computing setup
with the following specifications:

\begin{itemize}
      \item CPU: AMD Epyc 7642 48-Core, 96~thread processor
      \item RAM: 256 GB DDR4 3200 MHz
      \item GPU: NVIDIA GeForce RTX 3090 with 24 GB VRAM
      \item Storage: 2 TB NVMe SSD
\end{itemize}

\subsection{Evaluation protocol}

\subsubsection{Test set}

Evaluation was performed on a held-out test set of 100~phrases from the Liepa~2
dataset.

\subsubsection{Objective evaluation}

\subsubsection{Subjective evaluation}

Subjective evaluation was conducted using Mean Opinion Score (MOS) tests.

\section{Results and analysis}

\subsection{Tacotron~2 with DDC}

\subsubsection{Objective results}

\subsubsection{Subjective results}

\subsubsection{Qualitative analysis}

\subsection{FastPitch}

\subsubsection{Objective results}

\subsubsection{Subjective results}

\subsubsection{Qualitative analysis}

\section{Conclusion}

\subsection{Summary of findings}

\subsection{Contributions}

\subsection{Limitations of the study}

\subsection{Future work}

\section{\phantom{Appendix} References}

% In the document \textit{bibliography.bib}, you need to add all the cited sources and after using the function \textit{\{\textbackslash cite\{name of the cited object\}\}} the corresponding source will be added to the list of literature sources.

% \textit{bibliography.bib} provides examples of some of the most commonly cited types of sources:
% \begin{itemize}
%     \item web pages (\textit{@online}) \cite{PvzInternetinisPuslapis},
%     \item datasets (\textit{@dataset}) \cite{dataset}
%     \item articles (\textit{@article}) \cite{PvzStraipsnLt, PvzStraipsnEn}, 
%     \item articles from conferences (\textit{@inproceedings}) \cite{PvzKonfLt, PvzKonfEn}, 
%     \item books (\textit{@book}) \cite{PvzKnygLt, PvzKnygEn}, 
%     \item theses (\textit{@thesis or mastersthesis/phdthesis} \cite{PvzMagistrLt, PvzPhdEn})
%     \item electronic publications (\textit{@misc}) \cite{PvzElPubLt, PvzElPubEn}
% \end{itemize}

% Examples are also provided for ChatGPT citation, both in general \cite{chatgpt_bendrai} and for a specific conversation \cite{chatgpt_pokalbis}.

\end{document}
