%%%%%
%%%%%  Use LUALATEX, not LATEX.
%%%%%
%%%%
\documentclass[]{VUMIFTemplateClass}

\usepackage{indentfirst}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{mathtools}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage[hidelinks]{hyperref}
\usepackage{color,algorithm,algorithmic}
\usepackage[nottoc]{tocbibind}
\usepackage{tocloft}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}

\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage}

\makeatletter
\renewcommand{\fnum@algorithm}{\thealgorithm}
\makeatother
\renewcommand\thealgorithm{\arabic{algorithm} algorithm}

\usepackage{biblatex}
\bibliography{bibliografija}
%% to change the numbering (numeric or alphabetic) of bibliographic sources, make the change in VUMIFTemplateClass.cls, line 139

% Author's MACROS
\newcommand{\EE}{\mathbb{E}\,} % Mean
\newcommand{\ee}{{\mathrm e}}  % nice exponent
\newcommand{\RR}{\mathbb{R}}

\studyprogramme{Data Science}
\worktype{Master's thesis}
\worktitle{Data Selection Strategies for Multi-Speaker Text-to-Speech Synthesis in Lithuanian}
\secondworktitle{Work Title in Lithuanian}
\workauthor{Aleksandr Jan Smoliakov}

\supervisor{Dr.~Gerda Ana Melnik-Leroy}
\reviewer{pedagogical/scientific title Name Surname}
\scientificadvisor{Dr.~Gražina Korvel}

\begin{document}
\selectlanguage{english}

\onehalfspacing
\input{TitlePage}

% %% TODO Acknowledgements Section
% \sectionnonumnocontent{Acknowledgements}
% The author is thankful the Information Technology Research Center, Faculty of Mathematics and Informatics, Vilnius University, for providing the High-Performance Computing (HPC) resources for this research.
% %%
% %%
% %%      If you have used IT resources (CPU-h, GPU-h, other IT resources) provided by MIF for your thesis research, please leave the acknowledgement; if you have not, you can delete it.
% %%
% %%

% You can also add here acknowledgements for various other things, such as your supervisor, university, company, etc.

\singlespacing
\selectlanguage{english}

% list of figures, delete if not needed
\listoffigures

% list of tables, delete if not needed
\listoftables

% Table of contents
\tableofcontents
\onehalfspacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\sectionnonum{Introduction}

The goal of creating machines that can speak like humans has captivated
researchers for centuries. One of the earliest known attempts dates back to the
18th century, with Wolfgang von Kempelen's mechanical speech machine that
utilized a bellows-driven lung and physical models of the tongue and lips.

Over the centuries, advancements in technology and understanding of human
speech have driven significant progress in this field. Today's state-of-the-art
systems, dominated by end-to-end (E2E) neural models, have achieved highly
naturalistic speech with unprecedented acoustic quality. Notably, these
end-to-end systems have unified the entire synthesis process into a single
neural network, eliminating the need for complex multi-stage pipelines.

% \subsection{Background and motivation}

% \subsection{Problem statement}

Training high-quality TTS models typically requires large amounts of annotated
speech data. The common recommendation is to use at least 10~hours of recorded
speech from a single speaker to achieve good results.

Liepa~2~\cite{liepa2project} is a recently released Lithuanian speech corpus
that contains 1000~hours of annotated speech; however, this data is distributed
across more than 2600~speakers, with most speakers contributing only a few
minutes of speech. The top speaker has around 2.5~hours of recorded speech.

Training a high-quality single-speaker TTS model on such limited data poses a
challenge. Multi-speaker TTS models can utilize data from multiple speakers to
improve performance. However, training on all available data is a
time-consuming and computationally expensive process, especially in the context
of a master's thesis.

Therefore, it makes sense to explore strategies for selecting smaller subsets
of the available data for training. The question that arises is, what is the
best way to sample multi-speaker data for training TTS models?

% \subsection{Research questions}

This thesis aims to answer the following research questions:

\begin{itemize}
      \item TODO
\end{itemize}

% \subsection{Objectives}

% \subsection{Scope of the study}

Scope: This study is exclusively focused on the Lithuanian language and the
Liepa~2 speech corpus. It investigates a fixed total training data size of
30~hours. The models are limited to Tacotron~2 with DCA and FastPitch
architectures within the Coqui TTS framework, using a pre-trained WaveGlow
vocoder for waveform generation.

Limitations: The findings may not generalize to other languages, datasets with
different characteristics, or other TTS architectures. The 30-hour training
data size is a practical constraint and may not reflect performance at larger
scales.

% \subsection{Thesis structure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Literature review
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Literature review}

\subsection{Digital representation of audio}

Speech, or sound in general, is a continuous pressure wave that propagates
through a medium, such as air. The key properties of sound waves include
frequency (pitch), amplitude (loudness), and phase.

Converting continuous sound waves into a digital format suitable for computer
processing involves two main steps: sampling and quantization.

Sampling is the process of measuring the amplitude of the sound wave at regular
time intervals. The rate at which these samples are taken is called the
sampling rate. According to the Nyquist-Shannon~\cite{shannon1949communication}
sampling theorem, accurate reconstruction of a continuous signal requires a
sampling rate that is strictly greater than twice the highest frequency present
in the signal. Frequencies in the range between 300~Hz and 3400~Hz contribute
most to human speech intelligibility and
recognition.~\cite{jothilakshmi2016large}. In text-to-speech applications,
common sampling rates for audio are 22.05~kHz and 24~kHz, which can capture
frequencies up to approx. 11~kHz and 12~kHz, respectively.

Quantization (also known as bit depth) is the mapping of continuous amplitude
values to discrete levels for digital representation, which determines the
precision of the representation. Common bit depths for audio are 16-bit and
24-bit formats. A visual representation of both sampling and quantization is
provided in Figure~\ref{fig:sampling}.

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.8\textwidth]{figures/sampling_quantization.pdf}
      \caption[Visual representation of Analog-to-Digital conversion]{Visual representation of Analog-to-Digital conversion. The continuous grey line represents the analog signal. The vertical lines represent the \textbf{sampling rate} (time intervals), and the horizontal grid lines represent \textbf{quantization levels} (bit depth).}
      \label{fig:sampling}
\end{figure}

Pre-emphasis is a high-frequency filtering technique applied to audio signals
before further processing. Natural speech signals tend to have more energy in
the lower frequencies, with a gradual drop-off towards higher frequencies
(typically around -6~dB per octave). Pre-emphasis compensates for this spectral
tilt by boosting high frequencies using a first-order high-pass filter, which
is defined as:

\begin{equation}
      y[n] = x[n] - \alpha x[n-1]
\end{equation}

where \( y[n] \) is the pre-emphasized signal, \( x[n] \) is the original
signal, \( \alpha \) is the pre-emphasis coefficient (typically between 0.9 and
1.0, and often set to 0.97), and \( n \) is the sample index.

This transformation balances the frequency spectrum, improving the
signal-to-noise ratio for higher frequencies and preventing the model from
optimizing only for low-frequency components.

\subsection{Time-Frequency Analysis}

\subsubsection{Fourier Transform}

Fourier Transform (FT) is a mathematical technique that transforms a
time-domain signal (such as an audio waveform) into its frequency-domain
representation. The signal is decomposed into a sum of sine and cosine waves at
various frequencies, each with a specific amplitude and phase. This allows us
to analyze the frequency content of the signal.

Short-Time Fourier Transform~\cite{gabor1946theory} (STFT) extends the FT by
applying it to short, overlapping segments (frames) of the signal. This
transformation provides a time-frequency representation, showing how the
frequency content of the signal changes over time.

In TTS applications, the STFT is computed by dividing the audio signal into
short frames (usually, 20-50~ms) with a certain overlap (usually, 50-75\%)
between frames, windowed by a Hamming or Hann function to reduce the spectral
leakage.

\subsubsection{Spectrogram and Mel-spectrogram}

The spectrogram is a visual representation of the STFT, displaying frequency on
the vertical axis, time on the horizontal axis, and amplitude represented by
the color intensity.

However, the human ear does not perceive frequencies linearly --- it is more
sensitive to lower frequencies than higher ones. To mimic this perceptual
characteristic, the Mel scale~\cite{stevens1937scale} maps linear frequency \(
f \) (in Hz) to a perceptual scale \( m \) (in Mels) using the following
formula:

\begin{equation}
      m = 2595 \cdot \log_{10}\left(1 + \frac{f}{700}\right)
\end{equation}

Mel-spectrograms are computed by applying a Mel filterbank of overlapping
triangular filters (or kernels) to the magnitude spectrogram obtained from the
STFT. This results in a compressed representation of the audio signal that
aligns more closely with human auditory perception. Such Mel-spectrograms are
commonly used as input features for modern TTS systems. The differences between
the raw waveform, the standard spectrogram, and the Mel-spectrogram are
illustrated in Figure~\ref{fig:waveform_spectrograms}. Note how the
Mel-spectrogram has a higher resolution in the lower frequencies, where the
majority of the speech energy is concentrated.

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.8\textwidth]{figures/waveform_spectrograms.pdf}
      \caption[Raw waveform, Spectrogram, and Mel-spectrogram]{Raw audio waveform (top), its spectrogram (middle), and Mel-spectrogram (bottom) representations for the utterance ``Štai ir visas mano bendravimas su vaiku''.}
      \label{fig:waveform_spectrograms}
\end{figure}

\subsection{Text-to-speech synthesis}

Text-to-Speech (TTS) synthesis, also known as speech synthesis, is the process
of converting written text into human-like spoken words. Nowadays TTS is a key
technology in numerous applications, including virtual assistants,
accessibility tools, and language learning platforms.

\subsubsection{Traditional TTS approaches}

Early attempts at artificial speech synthesis evolved from the first mechanical
devices in the 18th century to electronic systems. Wolfgang von Kempelen's
mechanical speech machine demonstrated basic phoneme production using a
physical model of the vocal tract. In 1939, Homer Dudley's invention of the
Voder~\cite{klatt1987review} became the first electronic speech synthesizer
that could produce intelligible speech through operator-controlled acoustic
parameters, establishing the foundation for modern electronic synthesis
methods.

In the decades that followed, two main approaches for speech synthesis emerged:
concatenative synthesis and parametric synthesis.

\subsubsection{Concatenative synthesis}

The concatenative synthesis approach~\cite{hunt1996unit} synthesizes speech by
piecing together pre-recorded segments of human speech. This method involves
several steps. First, it requires pre-recording a large database of speech
segments spoken by a human voice actor in pristine, highly controlled studio
conditions to ensure consistent audio quality and minimize background noise.
Each segment is labeled and indexed based on its phonetic and prosodic
properties.

During synthesis, the system breaks down the input text into short linguistic
units (such as phonemes or syllables) using a text analysis module. Then, it
queries the speech database to find the best-matching segments for each unit
using selection cost functions~\cite{black1996optimising}. The retrieved
segments are blended and concatenated to form a continuous speech waveform.
Finally, the system uses signal processing techniques to smooth the transitions
between segments and adjust pitch and duration to match the desired output
characteristics.

Concatenative synthesis can produce natural-sounding individual speech units,
but the final audio often has noticeable audible continuity distortions at the
concatenation points~\cite{black1996optimising}. The segments may not blend
smoothly due to differences in pitch, duration, and timbre. The prosody also
tends to sound ``choppy'' and unnatural, since stringing disjointed segments
together does not capture the natural rhythm and intonation patterns of
connected speech.

Finally, concatenative synthesis requires language-specific expertise to design
and maintain the underlying speech database and selection algorithms. This need
for extensive data can make it challenging to develop concatenative TTS systems
for low-resource languages or dialects.

\subsubsection{Parametric synthesis}

In contrast, statistical parametric speech synthesis~\cite{zen2009statistical}
(SPSS) uses statistical models, typically Hidden Markov Models
(HMMs)~\cite{tokuda2013speech}, to generate the parameters that control a
speech waveform.

This method involves training a statistical model on a large corpus of recorded
speech. The model learns the relationship between linguistic features (like
phonemes and prosody) and the acoustic features of the speech signal, such as
spectral envelope and fundamental frequency. During synthesis, the system takes
text as input, converts it to a sequence of linguistic features, and then uses
the trained model to generate a corresponding sequence of acoustic parameters.

Compared to concatenative synthesis, the statistical approach allows for more
flexibility and control over the speech synthesis process, enabling the
generation of a wider variety of voices and speaking styles. However, HMM-based
synthesis~\cite{tokuda2013speech} had a persistent problem: the statistical
averaging built into the models tended to over-smooth the acoustic features,
creating the characteristic ``buzzy'' or ``muffled'' sound that lacked the
sharpness and detail of natural human speech.

\subsection{Linguistic Representation (Text Processing)}

In TTS systems, the input text must be pre-processed and converted into a
suitable linguistic representation that the synthesis model can use. The main
goal is to map the raw sentences into a sequence of symbols that can be more
closely mapped to the acoustic features of speech.

Although theoretically an end-to-end TTS model could learn to map raw text
directly to audio, in practice, pre-processing the text makes the model
convergence easier and improves the quality of the synthesized speech.

This process typically involves several steps, such as text normalization,
grapheme-to-phoneme conversion, and possibly prosody prediction.

\subsubsection{Text normalization}

Text normalization~\cite{sproat2001normalization} is the process of converting
raw written text with non-standard words (NSWs) into a more standardized
``spoken'' form. Typical steps include expanding abbreviations (e.g., expanding
``Dr.'' to ``Doctor''), punctuation removal, number normalization (e.g.,
converting ``123'' to ``one hundred twenty-three''), and lowercasing.

As an example, the input text ``Dr. Smith has 2 cats.'' could be normalized to
``doctor smith has two cats''.

Text normalization helps reduce the variability and complexity in the input
text, decreases the number of unique symbols, and removes the ambiguities that
could confuse the TTS model. The resulting normalized text is not only easier
for the model to process, but can also be further converted into phonemes,
which provide an even closer representation of the spoken language.

\subsubsection{Graphemes vs. Phonemes}

Text-to-speech systems use a discrete input representation derived from text,
generally divided into grapheme-based or phoneme-based sequences.

Grapheme-based models ingest raw character sequences (orthography). This
approach simplifies the inference pipeline by eliminating the dependency on
external grapheme-to-phoneme (G2P) converters. However, it forces the model to
implicitly learn pronunciation rules from data, which can be a significant
challenge for languages with complex orthographies or inconsistent
grapheme-to-phoneme mappings (e.g., ``read'' vs. ``read'').

In contrast, the phoneme-based approach uses a phonetic transcription of the
text, typically in the International Phonetic Alphabet (IPA) or ARPABET form.
By resolving pronunciation ambiguities prior to training, phonemes provide a
more direct mapping to acoustic features, simplifying the model's task of
learning alignment. The downside is that this approach requires an external
grapheme-to-phoneme (G2P) conversion step~\cite{bisani2008joint}. Additionally,
errors in the G2P conversion can propagate to the TTS model, affecting the
quality of the synthesized speech.

There is another approach that augments the grapheme-based representation with
explicit lexical stress markers or diacritics (e.g., tilde, acute, grave
accents). This intermediate method helps the model disambiguate pronunciation
of homographs and easier learn prosodic patterns without requiring a full
phonetic transcription, particularly in languages where stress placement alters
meaning.

\subsubsection{Specific challenges in Lithuanian}

Lithuanian is a Baltic language with a rich inflectional morphology and complex
prosodic structure. It is a pitch-accent language with free stress, meaning the
stress can fall on any syllable in a word, and can change the position
depending on the grammatical form.

Challenges in Lithuanian TTS synthesis include:

\begin{itemize}
      \item \textbf{High OOV rate:} Due to extensive word inflection, the number of unique word forms is significantly higher than in English. This leads to data sparsity issues where many valid word forms may not appear in the training set.
      \item \textbf{Ambiguity without accentuation:} Typically, stress marks are omitted in written Lithuanian. However, stress position and tone (acute, circumflex, or short) determine the meaning of monographic words. Examples are shown in Table~\ref{tab:lithuanian_ambiguity}. A grapheme-based model with accentuation marks has been shown to improve synthesis quality in Lithuanian.~\cite{kasparaitis2023investigation}.
\end{itemize}

\begin{table}[ht]
      \centering
      \begin{tabular}{|l|c|c|}
            \toprule
            \textbf{Word}          & \textbf{Accentuation}       & \textbf{Meaning}      \\
            \midrule
            \multirow{2}{*}{Antis} & \textit{ántis} (Acute)      & A duck (noun)         \\
                                   & \textit{añtis} (Circumflex) & Bosom/Chest (noun)    \\
            \midrule
            \multirow{2}{*}{Kasa}  & \textit{kãsa} (Circumflex)  & He/she digs (verb)    \\
                                   & \textit{kasà} (Short)       & Braid/Pancreas (noun) \\
            \bottomrule
      \end{tabular}
      \caption[Lithuanian homographs with accentuation ambiguity]{Examples of Lithuanian homographs where accentuation determines meaning. A grapheme-only model cannot distinguish these without context or explicit stress marks.}
      \label{tab:lithuanian_ambiguity}
\end{table}

To overcome these challenges, tools like
\textbf{Kirčiuoklis}~\cite{kirciuoklis} (Vytautas Magnus University) are often
employed in the text normalization pipeline. Kirčiuoklis automatically assigns
stress marks to raw text. One weakness of Kirčiuoklis is that it relies on
simple word-dictionary based lookup, which does not take into account the
context of the word. Thus, it suggests multiple possible accentuation variants
for homographs, leaving it up to the user to select the correct one.

In the absence of a high-quality, context-aware Grapheme-to-Phoneme (G2P)
converter for Lithuanian, this thesis will focus on grapheme-based TTS
synthesis with accentuation marks provided by Kirčiuoklis. In cases where
Kirčiuoklis suggests multiple accentuation variants for a word, no stress marks
will be added, leaving the TTS model to infer the correct prosody from context.

\subsection{Embeddings and Representation Learning}

\subsubsection{The Concept of Embeddings}

In machine learning, embeddings are dense vector representations of discrete
entities (such as words, characters, or speakers) to a high-dimensional
continuous vector space. Unlike one-hot encodings, which are sparse and highly
dimensional, embeddings provide a dense, lower-dimensional representation that
captures semantic relationships between underlying entities. For instance, in
word embeddings, similar words tend to have more similar (correlated) vector
representations, while dissimilar words map to more distant points in the
vector space.~\cite{mikolov2013efficient}

\subsubsection{Text Embeddings}

The ``Encoder'' part of a TTS model is responsible for converting a sequence of
input symbols (characters or phonemes) into a sequence of feature vectors.
Usually, this is done using an embedding layer, which maps each ``categorical''
input symbol to a learnable fixed-size vector representation. During training,
these embeddings are learned jointly with the rest of the TTS model.

\subsection{Deep learning for TTS}

The limitations of complex, multi-stage pipelines motivated the creation of the
end-to-end (E2E) model. E2E systems learn the entire speech synthesis process
--- from input text directly to acoustic output --- using a single neural
network. This approach promised to eliminate the need for hand-crafted
pipelines that were difficult to design, required extensive expertise, and
suffered from errors that accumulated across multiple components. By learning
directly from text-audio pairs, E2E models showed they could produce speech
with higher naturalness and expressiveness than previous methods, representing
a significant leap in TTS technology.

Although deep learning TTS models are more robust to variations in data quality
compared to concatenative approaches, they are essentially ``data-hungry''
systems that require large amounts of training data to achieve optimal
performance. Extrapolating from results in language modeling, it is observed
that model performance follows general scaling laws~\cite{kaplan2020scaling},
improving as the amount of training data increases.

However, in the context of multi-speaker synthesis, there is a trade-off
between the breadth of the data (number of distinct speakers) and the depth of
the data (duration of audio per speaker). In theory, training on a dataset with
a massive number of speakers, even with limited data per speaker, may allow the
model to learn a more generalized latent space of voice characteristics. This
high variance in the training data could act as a form of regularization,
preventing overfitting to noise and idiosyncrasies of individual speakers. In
contrast, datasets with fewer speakers but high duration per speaker allow the
model to capture fine-grained prosodic details specific to those voices,
potentially achieving higher stability but lower generalization capabilities.

\subsubsection{Feedforward neural networks}

Feedforward Neural Networks (FNNs) are the simplest type of artificial neural
networks, consisting of layers of interconnected nodes (neurons) where
information flows in one direction --- from the input, through hidden layers,
to the output. While FNNs can be useful for basic regression or classification
tasks, they lack the memory and context-awareness needed for processing
sequential data like text and speech. Therefore, FNNs are not suitable for
modelling TTS tasks that require understanding of temporal dependencies.

\subsubsection{Encoder-Decoder architectures}

The Encoder-Decoder architecture is a neural network architecture consisting of
two components, namely an encoder and a decoder. The encoder processes the
input data and compresses it into a high-dimensional latent representation.
This vector captures the meaningful features of the input. The decoder uses
this latent representation as context to generate the final output. This
architecture is commonly used in sequence-to-sequence tasks, such as machine
translation (text-to-text) and text-to-speech synthesis (text-to-audio frames).

\subsubsection{Sequence-to-Sequence models and Tacotron~2}

A common approach in modern neural TTS is the sequence-to-sequence (seq2seq)
framework~\cite{sutskever2014sequence}, which uses an encoder-decoder
architecture with an attention mechanism to map input (text) sequences to
output (audio) frames.

Tacotron~\cite{wang2017tacotron} and Tacotron~2~\cite{shen2018natural} are two
notable TTS models based on the sequence-to-sequence architecture. The variant
that this thesis primarily focuses on is Tacotron~2 with Dynamic Convolutional
Attention (DCA). The complete architecture of Tacotron~2 is depicted in
Figure~\ref{fig:tacotron_arch}.

This architecture has three main components:

\begin{enumerate}
      \item \textbf{Encoder:} The encoder's input is a character or phoneme sequence. A stack of convolutional layers followed by a bidirectional LSTM converts the character sequence into a high-level hidden feature representation.
      \item \textbf{Attention mechanism:} A location-sensitive attention~\cite{chorowski2015attention} mechanism learns to align the high-level text representation with the decoder steps. This alignment determines which parts of the input text should be attended to when generating each of the output audio frames.
      \item \textbf{Decoder and Post-net:} The autoregressive LSTM decoder generates a coarse Mel-spectrogram frame. This output is then passed through a convolutional \textbf{Post-net} which predicts a residual to refine the spectral details and improve reconstruction quality.
      \item \textbf{Stopnet:} A linear layer projects the LSTM decoder's output to a scalar, predicting the probability that the current frame is the ``stop token'', which halts the synthesis process. This allows the model to dynamically determine the output duration.
\end{enumerate}

The model is optimized by minimizing a combination of losses: the mean squared
error (MSE) between the predicted and ground truth Mel-spectrograms (both the
Decoder and Post-net outputs), the spectral similarity index (SSIM) loss for
improving spectral similarity, the ``guided attention'' loss to encourage
diagonal attention alignments, and the binary cross-entropy loss for the stop
token prediction.

While Tacotron~2 can generate high-quality speech, its autoregressive nature
makes inference slow, as each audio frame must be generated sequentially.
Additionally, the attention mechanism can sometimes fail, leading to issues
like skipped or repeated words in the synthesized speech.

\begin{figure}[ht]
      \centering
      % \includegraphics[width=\textwidth]{}
      \caption[Tacotron~2 architecture]{The Tacotron~2 architecture. Note the recurrent connections in the decoder and the attention mechanism aligning encoder outputs to decoder steps.~\cite{shen2018natural}}
      \label{fig:tacotron_arch}
\end{figure}

\subsubsection{Non-autoregressive models and FastPitch}

To address the slow inference speed and stability issues of autoregressive
models, non-autoregressive (parallel) models were developed.
FastPitch~\cite{lancucki2020fastpitch} is a notable example of such a model
that replaces the recurrent layers with Transformer~\cite{vaswani2023attention}
architecture blocks relying on self-attention.

Unlike Tacotron~2, FastPitch model generates the entire Mel-spectrogram in
parallel, significantly speeding up the inference. It utilizes a feed-forward
Transformer encoder and decoder. A key component of FastPitch is the explicit
modeling of prosody through pitch and duration predictors:

\begin{itemize}
      \item \textbf{Duration predictor:} Since the input text length does not match the output audio length, FastPitch requires an external aligner (or unsupervised alignment learning) to train a duration predictor. This module predicts how many audio frames correspond to each input character.
      \item \textbf{Pitch predictor:} A separate module predicts the fundamental frequency ($F_0$) for every character. This pitch contour is projected and added to the latent representation before decoding.
\end{itemize}

The explicit duration predictor allows FastPitch to control the length of the
generated speech and upsample the encoder output to match the target
Mel-spectrogram length. The pitch predictor enables explicit control over
intonation.

The high-level architecture of FastPitch is shown in
Figure~\ref{fig:fastpitch_arch}.

\begin{figure}[ht]
      \centering
      % \includegraphics[width=\textwidth]{figures/fastpitch_arch.pdf}
      \caption[FastPitch architecture]{The FastPitch architecture. It utilizes a feed-forward Transformer and explicit duration and pitch predictors, allowing for parallel generation of the Mel-spectrogram.~\cite{lancucki2020fastpitch}}
      \label{fig:fastpitch_arch}
\end{figure}

The primary advantages of FastPitch over Tacotron~2 are inference speed (due to
non-autoregressive generation), robustness (no attention failures like skipping
or repeating words), and controllability (pitch and speed can be tweaked
manually during synthesis).

\subsubsection{Other notable TTS models}

Besides Tacotron~2 and FastPitch, other notable TTS architectures include
\textbf{Glow-TTS}~\cite{kim2020glowtts}, which uses flow-based generative
models for parallel inference, and \textbf{VITS}~\cite{kim2021conditional}
(Conditional Variational Autoencoder with Adversarial Learning), which combines
the acoustic TTS model (Glow-TTS) with a neural vocoder (HiFi-GAN) into a
single end-to-end architecture.

\subsubsection{Neural vocoders}

\begin{figure}[ht]
      \centering
      \includegraphics[width=\textwidth]{figures/tts_pipeline.pdf}
      \caption[Text-to-Speech synthesis pipeline]{Text-to-Speech synthesis pipeline. The TTS model generates Mel-spectrograms from input text, which are then converted to raw audio waveforms by a neural vocoder.}
      \label{fig:tts_pipeline}
\end{figure}

As illustrated in Figure~\ref{fig:tts_pipeline}, modern TTS systems typically
employ a two-stage pipeline: an acoustic model like Tacotron~2 and FastPitch
generate intermediate acoustic features (Mel-spectrograms), but do not directly
produce raw audio waveforms. Spectrograms are lossy representations that only
capture the magnitude of the sound frequency bands, discarding phase
information. Converting a lossy spectrogram into audio is a non-trivial task,
as the phase information must be estimated. This challenge is known as the
\textit{inversion problem}.

An additional component called a vocoder is required to reconstruct the raw
waveform from the Mel-spectrogram.

Traditionally, the Griffin-Lim algorithm~\cite{griffin1984signal} has been used
to iteratively estimate and reconstruct the phase information from the
magnitude spectrogram. However, this method often produces audio with
noticeable artifacts and lower quality compared to natural speech.

Modern TTS systems use neural vocoders, which are deep generative models
trained to map acoustic features to raw waveforms.
\textbf{WaveNet}~\cite{oord2016wavenet} was one of the first autoregressive
models to produce high-fidelity audio, but its sequential generation process
made it prohibitively slow for real-time applications.

To address the speed limitations, Generative Adversarial Network (GAN) based
vocoders were introduced. \textbf{HiFi-GAN}~\cite{kong2020hifi} is currently
one of the state-of-the-art neural vocoders. It consists of a Generator that
upsamples the Mel-spectrograms using transposed convolutions and a set of
Discriminators (multi-scale and multi-period discriminators) that ensure the
generated audio is indistinguishable from real human speech. HiFi-GANs are
highly efficient and capable of faster-than-real-time synthesis on consumer
hardware while maintaining high perceptual quality.

The framework used in this thesis, Coqui TTS~\cite{coqui2021}, comes with a
pre-trained HiFi-GAN v2 vocoder trained on a large multi-speaker dataset
(VCTK~\cite{veaux2019cstr}) with 110 English speakers.

The use of a vocoder trained on English data for Lithuanian synthesis is
justified by the language-agnostic nature of the phase reconstruction task.
Neural vocoders' primary function is to model the physics of human speech
production rather than linguistic features. While language-dependent phonetic
nuances exist, studies have shown that vocoders trained on large, diverse
datasets can effectively generalize to unseen speakers and
languages~\cite{lorenzo2019towards}. Therefore, this thesis will utilize the
pre-trained HiFi-GAN~v2 model for waveform generation.

This should allow the vocoder model to effectively generalize to unseen
speakers and languages, provided that the acoustic feature parameters
(including sampling rate, FFT size, Mel-filterbank limits) of the input
Mel-spectrograms match those used during the vocoder's training.

Therefore, this thesis will utilize the pre-trained HiFi-GAN v2 model for
waveform generation. The TTS models' acoustic parameters will be configured to
the exact same acoustic parameters used during the vocoder's training to ensure
compatibility.

\subsection{Multi-speaker TTS}

Multi-speaker TTS models are designed to synthesize speech in the voices of
multiple speakers. In order to achieve this, these models are indeed trained on
data from many different speakers, allowing them to learn the characteristics
of each voice and synthesize speech that sounds like a specific individual,
while still being able to generalize the shared linguistic and acoustic
patterns across speakers.

\subsubsection{Speaker embeddings}

To enable multi-speaker synthesis, TTS models require a representation of the
speaker's identity. In multi-speaker models, the network is conditioned on a
speaker embedding. The model learns a shared representation of phonetics (how
text maps to sound generally) while using an additional input --- the speaker
embedding --- to adjust the timbre and prosodic characteristics specific to a
voice.

Early successful implementations of this approach include Deep Voice
2~\cite{arik2017deep}, which demonstrated effective multi-speaker synthesis by
learning speaker-specific embeddings.

Nowadays there are several techniques for incorporating speaker embeddings into
TTS models:

\textbf{Lookup Tables (LUT):} Early multi-speaker approaches used simple, learnable embeddings
where each speaker ID is mapped to a unique vector. The vectors are initialized randomly and learned
jointly with the TTS model. While this method is straightforward
and efficient, it cannot generalize to speakers not seen during training.

\textbf{d-vectors and x-vectors:} Transfer learning
approaches~\cite{jia2019transfer} have demonstrated adapting speaker
verification models for multispeaker TTS synthesis, enabling better speaker
adaptation and higher voice quality. The general architecture of such a speaker
encoder is illustrated in Figure~\ref{fig:speaker_encoder}. A speaker encoder
model pre-trained on a massive, noisy dataset with thousands of speakers (e.g.,
the VoxCeleb dataset~\cite{nagrani2017voxceleb}) learns the general speaker
space. Its pre-trained weights are frozen and used to extract embeddings for
the TTS training data, allowing the TTS model to effectively account for
multi-speaker variation.

\begin{figure}[ht]
      \centering
      % \includegraphics[width=\textwidth]{figures/speaker_encoder_diagram.pdf}
      \caption[General architecture of a Speaker Encoder]{General architecture of a Speaker Encoder. A reference audio of arbitrary length is processed (typically by LSTM or TDNN layers) and pooled to produce a fixed-length embedding vector (e.g., d-vector) representing the speaker identity.}
      \label{fig:speaker_encoder}
\end{figure}

\textbf{d-vectors:} d-vectors~\cite{variani2014deep} are fixed-length speaker embeddings derived from a separate speaker verification model. A reference encoder network takes a reference audio recording of arbitrary length and compresses it into a fixed-length vector known as a d-vector, that summarizes the speaker's timbral and prosodic characteristics. These d-vectors are then provided as additional input to the TTS model, and are kept fixed during TTS training.

\textbf{x-vectors:} An evolution of d-vectors, x-vectors~\cite{snyder2018x} use a Time Delay Neural Network (TDNN) architecture to capture the temporal context more effectively. These embeddings have shown an improved ability in zero-shot TTS scenarios.

One limitation of d-vectors and x-vectors is that if the reference audio is of
poor quality or contains background noise, the resulting speaker embedding may
not accurately represent the speaker's identity, leading to degraded synthesis
quality.

% TODO How Coqui TTS handles speakers: Specifically, how speaker embeddings are
% concatenated or added to the encoder outputs to condition the synthesis on a
% specific voice identity.

\subsubsection{Challenges}

One key challenge in multi-speaker TTS is ensuring that the model can
generalize across many speakers while still maintaining high quality for each.
There is a trade-off between the \textit{breadth} of the dataset (number of
speakers) and the \textit{depth} (minutes of audio per speaker).

Standard TTS systems historically required 10 to 20 hours of recorded speech
for a single professional speaker. However, deep learning models capable of
\textit{transfer learning} can produce intelligible speech for a new speaker
with significantly less data, potentially as little as a few minutes --- if the
base model has been pre-trained on a sufficiently diverse multi-speaker
dataset.

\subsection{Evaluation metrics}

Evaluating Text-to-Speech systems is notoriously difficult because ``quality''
is a subjective metric defined by human perception. There is no single
mathematical objective function that perfectly correlates with human judgement
of naturalness and intelligibility. Therefore, TTS systems are typically
evaluated using subjective listening tests.

\subsubsection{Mean Opinion Score (MOS)}

The most standard metric for evaluating speech synthesis quality is the Mean
Opinion Score (MOS), originally derived from telecommunications quality
standards (ITU-T P.800).~\cite{itup800}.

In a MOS test, human listeners (raters) are presented with a set of synthesized
speech audio samples and asked to rate them on a 5-point Likert scale. The
standard scale for ``Naturalness'' is:

\begin{itemize}
      \item \textbf{5:} Excellent (Imperceptible difference from real speech)
      \item \textbf{4:} Good (Perceptible but not annoying)
      \item \textbf{3:} Fair (Slightly annoying)
      \item \textbf{2:} Poor (Annoying)
      \item \textbf{1:} Bad (Very annoying / Unintelligible)
\end{itemize}

The final score is the arithmetic mean of all ratings collected for a specific
TTS system. Although MOS is subjective, with a sufficient number of raters
(typically, at least 15-20), the scores tend to converge and provide a reliable
ranking between different models.

\subsubsection{Latin square design}

A major challenge in subjective listening tests is controlling for biases. If a
rater hears the same sentence produced by different TTS systems in a row, their
ratings may be influenced by the repetition (repetition effect) or by the
relative order of presentation (order effect). For instance, a ``Slightly
annoying'' sample may be rated more harshly if it follows an ``Excellent''
sample (contrast effect).

In order to mitigate these biases, a Latin square
design~\cite{williams1949experimental} is often employed for MOS tests. In this
experimental design:

\begin{enumerate}
      \item A set of test sentences (utterances) is selected.
      \item The listeners are divided into groups.
      \item The presentation is balanced such that each listener hears every test sentence
            exactly once, and every TTS system (model) exactly once per block of trials,
            but never the same sentence-system combination twice.
\end{enumerate}

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.6\textwidth]{figures/latin_square_design.pdf}
      \caption[Latin square design for TTS evaluation]{Latin square design for TTS evaluation. Each listener group hears each sentence exactly once, and each TTS system exactly once per block, ensuring balanced exposure and mitigating order/repetition biases.}
      \label{fig:latin_square}
\end{figure}

An example Latin square design for 4 TTS systems and 4 test sentences is
illustrated in Figure~\ref{fig:latin_square}.

For a multi-speaker TTS evaluation (as is the case in this thesis), the Latin
square design ensures that the ratings reflect the quality of the model rather
than the linguistic content of the sentence or listener fatigue. By rotating
the systems and sentences across listener groups, the influence of specific
difficult sentences is averaged out across all models.

\subsection{Research gap}

While the literature demonstrates the capabilities of modern deep learning TTS
architectures like Tacotron~2 and FastPitch to produce highly natural-sounding
speech, several questions remain unanswered regarding their application to
low-resource, morphologically complex languages like Lithuanian.

Firstly, although neural TTS models may follow general neural model scaling
laws~\cite{kaplan2020scaling}, implying that performance improves with more
data, there is limited understanding of the optimal composition of training
data under a fixed budget. In low-resource settings, scaling up the dataset
size is not always feasible, and this may be further constrained by the
computational resources required for training large models. A critical question
is whether it is more beneficial to train on a smaller number of speakers with
more data per speaker (high depth) or a larger number of speakers with less
data per speaker (high breadth).

Current research primarily focuses on high-resource languages like English,
where the availability of large, balanced multi-speaker datasets masks the
nuances of this trade-off. For a pitch-accent language like Lithuanian, the
requirements may be different. It is hypothesized that high-diversity datasets
may help the model learn a richer representation of prosodic patterns, while
high-depth datasets may improve the model's naturalness for the target
speakers.

Secondly, most multi-speaker TTS research assumes access to large-scale
datasets with thousands of utterances per speaker. There is a lack of research
exploring how different TTS architectures (autoregressive vs.
non-autoregressive) perform when the data per speaker is scarce (e.g., under 10
minutes).

This thesis aims to fill the research gap by systematically evaluating the
efficiency of Tacotron~2 and FastPitch models trained on Lithuanian speech
data. By controlling the total dataset size and varying the distribution of
speakers and data per speaker, this study will provide insights into the
optimal data composition for training multi-speaker TTS models in low-resource
settings.

To summarize, the key research questions this thesis seeks to answer are:

\begin{itemize}
      \item How does the trade-off between data breadth (number of speakers) and data depth
            (minutes per speaker) affect the performance of multi-speaker TTS models for
            Lithuanian?
      \item How do different TTS architectures (Tacotron~2 vs. FastPitch) perform under
            varying data selection strategies in low-resource settings?
\end{itemize}

The experiments will involve training models on three distinct data selection
strategies:

\begin{itemize}
      \item \textbf{High-resource per speaker:} Fewer speakers (30), but high fidelity (45 min each), total: 22.5 hours.
      \item \textbf{Balanced:} Moderate diversity (60 speakers), moderate data (22.5 min each), total: 22.5 hours.
      \item \textbf{High-diversity:} Many speakers (180), low resource (7.5 min each), total: 22.5 hours.
\end{itemize}

The extreme low-depth condition (7.5 minutes per speaker) might pose
convergence challenges for the models, especially for Tacotron~2, which relies
on learning robust attention alignment. Thus, alignment convergence and
training stability will be monitored to assess how data composition affects
model robustness.

\subsection{Summary}

This literature review has provided an overview of the theoretical foundations
required for modern Text-to-Speech synthesis. The evolution of TTS systems from
mechanical apparatuses, through concatenative and statistical methods, to
end-to-end deep learning architectures capable of generating natural-sounding
speech has been discussed.

We have reviewed the entire TTS pipeline --- from signal processing (sampling,
quantization, Fourier transforms, and Mel-spectrogram extraction), through text
normalization and representation (graphemes vs.\ phonemes), to deep learning
architectures for acoustic modeling and vocoding. The literature highlights two
architectures for acoustic modeling: the autoregressive Tacotron~2, known for
high-quality spectral output but slow inference and stability issues, and the
non-autoregressive FastPitch, which offers parallel generation with explicit
control over pitch and duration.

We have examined the challenges specific to Lithuanian TTS synthesis. Unlike
English, Lithuanian languages's high inflectional morphology leads to a large
number of unique word forms, and its prosodic system requires handling of free
stress and pitch accents, necessitating the use of tools like Kirčiuoklis for
accentuation marking.

Finally, we have reviewed the role of speaker embeddings in enabling
multi-speaker synthesis and the use of neural vocoders, specifically HiFi-GAN,
to reconstruct high-fidelity waveforms from Mel-spectrograms. Despite these
advancements, a gap remains in understanding how data diversity versus quantity
affects model performance for complex, low-resource languages --- a challenge
this thesis addresses through the experiments detailed in the following
chapters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Methodology
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

This chapter details the experimental setup, data processing pipeline, training
configurations, and evaluation protocol used to address the research questions
regarding the optimal composition of multi-speaker training data for Lithuanian
Text-to-Speech (TTS) synthesis. The experiments are designed to systematically
compare the performance of autoregressive and non-autoregressive models under
varying degrees of data breadth and depth, while controlling for the total
training budget.

\subsection{Research Design}

The study employs a factorial design to investigate the impact of data
distribution on synthesis quality.

\subsubsection{Variables}

\begin{itemize}
      \item \textbf{Independent Variables:}
            \begin{itemize}
                  \item \textbf{Data Selection Strategy:} Three distinct subsets varying in speaker count versus duration per speaker (Breadth vs. Depth).
                  \item \textbf{Model Architecture:} Autoregressive (Tacotron 2) vs. Non-autoregressive (FastPitch).
            \end{itemize}

      \item \textbf{Dependent Variables:}
            \begin{itemize}
                  \item \textbf{Objective Performance:} Mel-Cepstral Distortion (MCD) and Attention Alignment convergence.
                  \item \textbf{Subjective Performance:} Naturalness ratings via Mean Opinion Score (MOS).
            \end{itemize}

      \item \textbf{Controlled Variables:}
            \begin{itemize}
                  \item \textbf{Total Data Volume:} Fixed at 22.5 hours for all experiments.
                  \item \textbf{Source Domain:} Read speech (audiobooks, news) from the Liepa 2 corpus.
                  \item \textbf{Test Sentences and Speakers:} Same set of evaluation sentences and seen speakers across all models.
                  \item \textbf{Vocoder:} Pre-trained HiFi-GAN v2 used across all models.
            \end{itemize}
\end{itemize}

\subsection{Data and preprocessing}

\subsubsection{Liepa~2 dataset}

The foundation of this study is the \textbf{Liepa~2} Lithuanian speech
corpus~\cite{liepa2project}. The full corpus contains over 1000~hours of
recorded speech from 2621~speakers, along with corresponding text
transcriptions. The recordings span various speech styles and contexts,
including read speech (audiobooks, news), TV and radio broadcasts, spontaneous
speech.

To focus on adult speech patterns and ensure consistent acoustic quality, the
source data was filtered to include only ``read speech'' from adults (18+ years
old), excluding spontaneous speech and children.

Some key statistics of the full Liepa~2 dataset are as follows:
\begin{itemize}
      \item Number of speakers: 2,621~unique speakers.
      \item Number of utterances: 1,874,648~recorded utterances.
      \item Total duration: 1,000~hours and 45~minutes of recorded speech.
      \item Sampling rate: 16~kHz, 16-bit PCM WAV format.
\end{itemize}

\subsubsection{Speaker filtering criteria}

The Liepa~2 corpus presents a challenge as most speakers contribute only a few
minutes of audio. To ensure a valid comparison across data strategies, speakers
for each subset were selected based on the following criteria:

\begin{enumerate}
      \item \textbf{Speech type filtering}: Only ``read speech'' samples are used, excluding
            spontaneous speech to ensure consistent quality and pronunciation.
      \item \textbf{Age group filtering}: Speakers from age groups 18--25, 26--60, and 60+ are included, excluding children (0--17) to focus on adult speech patterns.
      \item \textbf{Minimum duration:} Only speakers with at least the required minimum duration for the specific experiment were selected until the required per-speaker duration was met.
      \item \textbf{Speaker subset hierarchy:} Each subset of speakers was also a superset of any other dataset with fewer speakers. For instance, the high-depth dataset's speakers (30) is a subset of the balanced dataset's speakers (60), which in turn is a subset of the high-breadth dataset's speakers (180). This selection ensures that the same speakers can be used for evaluation across all models.
      \item \textbf{Gender Balance:} The selection maintained an exactly equal split between male and female speakers to ensure the models generalize across pitch ranges.
\end{enumerate}

\subsection{Experimental Design: Data Subsets}

To investigate the impact of data selection on TTS performance, several
strategies for selecting subsets of the Liepa~2 dataset were employed. All
strategies maintain a fixed total training budget of 22.5~hours to ensure fair
comparison across experiments.

The configurations are defined in Table~\ref{tab:data_subsets}.

\begin{table}[ht]
      \centering
      \caption[Experimental Data Subsets]{Summary of the three experimental data subsets. The total duration is held constant while varying the number of speakers and the duration per speaker.}
      \label{tab:data_subsets}
      \begin{tabular}{lcccc}
            \toprule
            \textbf{Subset Name} & \textbf{Strategy} & \textbf{Speakers ($N$)} & \textbf{Time/Speaker} & \textbf{Total Time} \\
            \midrule
            \textbf{Set-Depth}   & High Fidelity     & 30                      & 45 min                & 22.5 hours          \\
            \textbf{Set-Balance} & Balanced          & 60                      & 22.5 min              & 22.5 hours          \\
            \textbf{Set-Breadth} & High Diversity    & 180                     & 7.5 min               & 22.5 hours          \\
            \bottomrule
      \end{tabular}
\end{table}

\subsubsection{Text Normalization and Accentuation}

Text data underwent normalization to ensure compatibility with the
grapheme-based training approach. Liepa~2 transcripts already include a fair
level of normalization --- for instance, the following elements are written in
full words: numbers, dates, abbreviations, acronyms, and symbols.

However, some additional normalization steps are applied to further standardize
the transcriptions:

\begin{enumerate}
      \item \textbf{Punctuation Standardization:} Rare and non-standard punctuation was mapped to standard set (.,-?!).
      \item \textbf{Removal of extraneous characters:} Any remaining characters that are not letters, digits, whitespace, or basic punctuation (.,-?! ) were removed.
      \item \textbf{Whitespace normalization:} Multiple consecutive whitespace characters were collapsed into a single space, and leading/trailing whitespace was trimmed.
      \item \textbf{Accentuation:} Raw text was processed using \textbf{Kirčiuoklis}~\cite{kirciuoklis} for automatic stress assignment. In cases where Kirčiuoklis suggested multiple stress possibilities for a homograph, the text was left unaccentuated, relying on the model to infer prosody from context.
      \item \textbf{Lowercasing:} All text was converted to lowercase to reduce the output space.
      \item \textbf{Character Substitution:} Non-Lithuanian letters were replaced with equivalents (e.g., `w' $\to$ `v', `x' $\to$ `ks').
\end{enumerate}

As a result of these normalization steps, the vocabulary size is reduced from
140~characters to 41~characters.

The final alphabet used for training consists of the following characters:
\begin{verbatim}
a ą b c č d e ę ė f g h i į y j k l m n o p r s š t u ų ū v z ž ´ ` ~ (space) . , - ? !
\end{verbatim}

\subsubsection{Audio Preprocessing}

Audio recordings were resampled from their original \textbf{16,000 Hz} to
\textbf{22,050 Hz}. While resampling to a higher frequency does not add new
information, the resampling was performed to match the pre-trained vocoder.
Leading and trailing silence was trimmed to remove non-speech segments.
Acoustic features were extracted using the parameters listed in
Table~\ref{tab:audio_params}.

\begin{table}[h!]
      \centering
      \caption{Mel-spectrogram extraction parameters.}
      \label{tab:audio_params}
      \begin{tabular}{lr}
            \toprule
            \textbf{Parameter} & \textbf{Value}        \\
            \midrule
            Sampling Rate      & 22,050 Hz             \\
            FFT Size           & 1024 samples (46 ms)  \\
            Hop Length         & 256 samples (11.6 ms) \\
            Window Length      & 1024                  \\
            Mel Channels       & 80                    \\
            Freq Range         & 0--8000 Hz            \\
            Pre-emphasis       & 0.98                  \\
            \bottomrule
      \end{tabular}
\end{table}

A complete list of audio parameters is presented in the
Appendix~\ref{appendix:audio_params}.

\subsubsection{Speaker embeddings}

The FastPitch model does not support using both speaker embeddings and external
d-vectors simultaneously. Therefore, for FastPitch training, only learnable
fixed-length speaker embeddings were used.

For Tacotron~2 training, both learnable speaker embeddings and external clip
embeddings were used.

The speaker identity was provided using fixed-length embeddings, specifically
\textbf{x-vectors}~\cite{snyder2018x}. These x-vectors were extracted using a
speaker encoder model pre-trained on a large, external multi-language dataset
(e.g., VoxCeleb) and were kept frozen during the TTS model training. The
x-vector for each utterance was concatenated to the output of the respective
acoustic model's encoder, allowing the decoder to condition the generated
Mel-spectrogram on the target speaker's voice characteristics. The speaker
embedding computation process involves:

\begin{itemize}
      \item \textbf{Speaker encoder model}: Pre-trained model from Coqui TTS based on the approach described in~\cite{jia2019transfer}.
      \item \textbf{Embedding extraction}: Each speaker's audio samples are processed
            through the speaker encoder to generate 512-dimensional speaker embeddings
            (d-vectors).
            These speaker embeddings are then used during TTS training to condition the
            model on speaker identity, enabling the synthesis of speech in different
            voices.
\end{itemize}

\subsection{Model Architectures}

Two distinct acoustic models were trained on each of the three datasets,
implemented using the \textbf{Coqui TTS}~\cite{coqui2021} framework.

\subsubsection{Tacotron~2 Configuration}

The autoregressive model used is \textbf{Tacotron~2}, modified with Dynamic
Convolutional Attention (DCA) to accelerate alignment convergence.
\begin{itemize}
      \item \textbf{Encoder:} 3-layer convolutional stack followed by a bidirectional LSTM (512 units).
      \item \textbf{Decoder:} 2-layer LSTM (1024 units) with location-sensitive attention.
      \item \textbf{Speaker Conditioning:} A learnable embedding layer (dim=256) concatenated with encoder outputs.
\end{itemize}

\subsubsection{FastPitch Configuration}

The non-autoregressive model used is \textbf{FastPitch}, a feed-forward
Transformer architecture.
\begin{itemize}
      \item \textbf{Backbone:} 6-layer Transformer encoder and 6-layer decoder (Hidden dim=384).
      \item \textbf{Alignment:} Trained using unsupervised Soft-DTW (Dynamic Time Warping) to generate duration targets without external aligners.
      \item \textbf{Variance Predictors:} Explicit 1D-convolutional predictors for pitch and duration.
      \item \textbf{Speaker Conditioning:} A learnable embedding layer (dim=512) added to encoder outputs.
\end{itemize}

\subsubsection{Vocoder}

Consistent with the literature review's justification for language-agnostic
generalization, a \textbf{HiFi-GAN v2}~\cite{kong2020hifi} model pre-trained on
the multi-speaker VCTK corpus~\cite{veaux2019cstr} was used for all
experiments. The vocoder weights remained fixed throughout the TTS training,
ensuring that differences in final audio quality were solely attributable to
the acoustic models.

\subsection{Training Procedure}

\subsubsection{Computational environment}

Experiments were conducted on a personal workstation equipped with the
following hardware:

\begin{itemize}
      \item CPU:\ AMD\@ Epyc 7642 48-core @ 2.3 GHz
      \item RAM:\ 256 GB\@ DDR4 3200~MHz
      \item GPU:\ NVIDIA\@ GeForce RTX\@ 3090 with 24 GB\@ VRAM
      \item Storage: 2 TB\@ NVMe SSD in RAID~5 configuration
\end{itemize}

The software environment included Ubuntu 25.04 LTS, Python 3.13.3, and Coqui
TTS v0.12.0. CUDA 13.0 was used for GPU acceleration.

The exact Python environment configuration is provided in the accompanying
GitHub repository.

\subsubsection{Implementation framework}

All experiments are implemented using the Coqui TTS framework, an open-source
toolkit for training TTS models. The experimental pipeline is automated using
Make build system with the following components:

\begin{itemize}
      \item \textbf{Data preprocessing:} A Python script for audio conversion,
            text normalization, and metadata generation.
      \item \textbf{Speaker embedding computation:} Batch processing of speaker
            embeddings using pre-trained encoder models.
      \item \textbf{Training orchestration:} Automated model training with
            hyperparameter configuration and checkpointing.
      \item \textbf{Inference pipeline:} Synthesis of test sentences for evaluation purposes.
      \item \textbf{Evaluation tools:} Integration with subjective evaluation web application and objective metric computation.
\end{itemize}

\subsection{Model training configurations}

\begin{itemize}
      \item \textbf{Gradient clipping}: 0.05 to prevent gradient explosion.
      \item \textbf{Validation}: 1\% of training data held out for validation.
      \item \textbf{Checkpointing}: Model checkpoints saved every 5000 steps, with the best checkpoint
            selected based on lowest validation loss.
\end{itemize}

Training time stability and attention alignment convergence were monitored,
particularly for the Set-Breadth Strategy C strategy to detect convergence
failures caused by data sparsity.

\subsubsection{Tacotron~2 with DCA hyperparameters}

\begin{table}[h!]
      \centering
      \caption[Tacotron~2 DCA training configuration]{Tacotron~2 DCA training configuration}
      \begin{tabular}{lr}
            \toprule
            \textbf{Parameter}    & \textbf{Value}      \\
            \midrule
            Batch size            & 64                  \\
            Learning rate         & 0.001               \\
            Optimizer             & RAdam               \\
            LR scheduler          & MultiStepLR         \\
            Max epochs            & 200 (~90,000 steps) \\
            Attention type        & Location-sensitive  \\
            Memory size           & -1 (disabled)       \\
            Speaker embedding dim & 512                 \\
            Number of speakers    & 30 / 60 / 180       \\
            Stopnet               & Enabled             \\
            Separate stopnet      & True                \\
            \bottomrule
      \end{tabular}
\end{table}

\textbf{Loss function}: Combination of decoder loss (α=0.25), post-net loss
(α=0.25), SSIM losses (α=0.25), guided attention loss (α=5.0), and stop token
loss (weight=15.0).

\subsubsection{FastPitch hyperparameters}

\begin{table}[h!]
      \centering
      \caption[FastPitch training configuration]{FastPitch training configuration}
      \begin{tabular}{lr}
            \toprule
            \textbf{Parameter}         & \textbf{Value}       \\
            \midrule
            Batch size                 & 32                   \\
            Eval batch size            & 16                   \\
            Learning rate              & 0.001                \\
            Optimizer                  & RAdam                \\
            LR scheduler               & MultiStepLR          \\
            Max epochs                 & 200 (~180,000 steps) \\
            Duration predictor layers  & 2                    \\
            Pitch predictor layers     & 2                    \\
            Transformer encoder layers & 6                    \\
            Transformer decoder layers & 6                    \\
            Attention heads            & 1                    \\
            Encoder hidden dim         & 384                  \\
            Decoder hidden dim         & 384                  \\
            \bottomrule
      \end{tabular}
\end{table}

\textbf{Loss function}: Combination of Mel-spectrogram MSE, duration predictor loss (MSE), and pitch predictor loss (MSE).

% TODO Learning rate scheduling using MultiStepLR (milestones at 20k, 30k, 40k, 50k
% steps with gamma=0.5).

\subsection{Evaluation protocol}

The synthesized speech from the trained models was evaluated using a
combination of objective and subjective metrics.

\subsubsection{Test set}

A held-out test set (20 standardized sentences, unseen during training) was
used for evaluation. The test set included speakers seen during training to
assess the models' ability to generalize to known voices.

\subsubsection{Objective evaluation}

Objective evaluation is performed using standard acoustic metrics. While useful
for monitoring training, these metrics do not perfectly correlate with human
perception and served primarily as diagnostic tools.

\begin{itemize}
      \item \textbf{Mel Cepstral Distortion (MCD):} Measures the spectral distance
            between synthesized and ground truth Mel-spectrograms, with lower values
            indicating better quality.
      \item \textbf{Fundamental Frequency RMSE (F0 RMSE):} Evaluates pitch accuracy
            by measuring the root mean square error between predicted and ground truth
            F0 contours.
      \item \textbf{Training convergence metrics:} Training and validation loss
            curves, attention alignment visualization, and convergence time.
\end{itemize}

\subsubsection{Objective Analysis}

In addition to MOS, the training stability was monitored. specifically for
Tacotron~2. The \textbf{attention alignment plots} were generated at regular
checkpoints. A failure to converge to a diagonal alignment indicates that the
model has failed to learn the text-to-audio mapping. This binary metric
(Converged / Failed) is crucial for the \textit{Set-Breadth} (7.5 min/speaker)
scenario, where data sparsity may prevent attention mechanisms from
stabilizing.

\subsubsection{Subjective Evaluation (MOS)}

Subjective evaluation was conducted through a web-based listening test
application developed specifically for this study.

The web-based interface used for the evaluation presented samples in a
randomized order, ensuring no rater could identify which model or dataset
produced the audio.

\begin{itemize}
      \item \textbf{Rating Scale:} Naturalness was rated using the standard 5-point Mean Opinion Score scale.
            \begin{itemize}
                  \item 5: Excellent (Imperceptible difference from real speech)
                  \item 4: Good (Perceptible but not annoying)
                  \item 3: Fair (Slightly annoying)
                  \item 2: Poor (Annoying)
                  \item 1: Bad (Very annoying / Unintelligible)
            \end{itemize}
      \item \textbf{Test Design:} A \textbf{Latin square design} was employed to mitigate order and repetition biases. The experiment included a set of 20 unique test sentences, synthesized by all experimental models (6 models: Tacotron 2 and FastPitch, each trained on three strategies, plus a baseline human recording).
      \item \textbf{Raters:} For the listening test, 20 native Lithuanian-speaking raters were recruited through university networks and social media platforms. Each rater evaluated a randomized block of sentences, ensuring balanced exposure to all models and sentences. The final MOS was calculated as the arithmetic mean of all collected ratings for each model.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Results and analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Analysis}

% This chapter presents the experimental results for the Tacotron~2 and FastPitch
% models across the three data selection strategies (\textit{Set-Depth},
% \textit{Set-Balance}, \textit{Set-Breadth}). It first examines training
% stability and alignment convergence, followed by objective acoustic metrics,
% and concludes with the subjective Mean Opinion Score (MOS) evaluation.

% \subsection{Training Stability and Convergence}

% Before evaluating audio quality, the models were assessed for their ability to
% learn stable text-to-audio alignments, particularly under the data-sparse
% \textit{Set-Breadth} condition.

% \subsubsection{Attention Alignment Analysis (Tacotron~2)}
% \begin{itemize}
%       \item \textbf{Convergence Rates:} Report on whether Tacotron~2 successfully converged to diagonal alignments for all three subsets.
%       \item \textbf{Failure Modes:} If the \textit{Set-Breadth} (7.5 min/speaker) caused alignment failures (e.g., repeating words, skipping text), detail them here.
%       \item \textbf{Visual Analysis:}
%             % [Insert Figure: Comparison of a sharp, diagonal alignment vs. a fuzzy/failed alignment]
% \end{itemize}

% \subsubsection{Duration Prediction Stability (FastPitch)}
% \begin{itemize}
%       \item \textbf{Soft-DTW Performance:} Report on whether the unsupervised alignment in FastPitch was robust to the reduction in data per speaker.
%       \item \textbf{Loss Convergence:} Briefly compare the training loss curves. Did the \textit{Set-Breadth} model struggle to minimize duration loss?
% \end{itemize}

% \subsection{Objective Evaluation}

% Table~\ref{tab:objective_results} summarizes the acoustic performance metrics
% evaluated on the held-out test set.

% \begin{table}[ht]
%       \centering
%       \caption[Objective Evaluation Results]{Comparison of Mel-Cepstral Distortion (MCD) and F0 RMSE across all experimental conditions. Lower values indicate better performance.}
%       \label{tab:objective_results}
%       \begin{tabular}{llcc}
%             \toprule
%             \textbf{Model Architecture} & \textbf{Data Strategy} & \textbf{MCD (dB) $\downarrow$} & \textbf{F0 RMSE (Hz) $\downarrow$} \\
%             \midrule
%             \multirow{3}{*}{Tacotron 2} & Set-Depth              & 0.00                           & 0.00                               \\ % Fill with data
%                                         & Set-Balance            & 0.00                           & 0.00                               \\
%                                         & Set-Breadth            & 0.00                           & 0.00                               \\
%             \midrule
%             \multirow{3}{*}{FastPitch}  & Set-Depth              & 0.00                           & 0.00                               \\
%                                         & Set-Balance            & 0.00                           & 0.00                               \\
%                                         & Set-Breadth            & 0.00                           & 0.00                               \\
%             \bottomrule
%       \end{tabular}
% \end{table}

% \begin{itemize}
%       \item \textbf{Spectral Accuracy (MCD):} Analysis of which strategy minimized spectral distortion. Did adding more speakers (Breadth) hurt the overall acoustic fidelity?
%       \item \textbf{Prosodic Accuracy (F0):} Analysis of pitch reconstruction accuracy.
% \end{itemize}

% \subsection{Subjective Evaluation (MOS)}

% The naturalness of the synthesized speech was evaluated by 20 native Lithuanian
% listeners. Table~\ref{tab:mos_results} presents the Mean Opinion Scores (MOS)
% with 95\% Confidence Intervals (CI).

% \begin{table}[ht]
%       \centering
%       \caption[Subjective MOS Results]{Mean Opinion Scores (MOS) for naturalness. $N=20$ raters.}
%       \label{tab:mos_results}
%       \begin{tabular}{llc}
%             \toprule
%             \textbf{System}       & \textbf{Strategy} & \textbf{MOS $\pm$ 95\% CI} \\
%             \midrule
%             \textit{Ground Truth} & \textit{--}       & \textit{4.XX $\pm$ 0.XX}   \\
%             \midrule
%             Tacotron 2            & Set-Depth         & 0.00 $\pm$ 0.00            \\
%             Tacotron 2            & Set-Balance       & 0.00 $\pm$ 0.00            \\
%             Tacotron 2            & Set-Breadth       & 0.00 $\pm$ 0.00            \\
%             \midrule
%             FastPitch             & Set-Depth         & 0.00 $\pm$ 0.00            \\
%             FastPitch             & Set-Balance       & 0.00 $\pm$ 0.00            \\
%             FastPitch             & Set-Breadth       & 0.00 $\pm$ 0.00            \\
%             \bottomrule
%       \end{tabular}
% \end{table}

% \subsubsection{Impact of Data Strategy}
% \begin{itemize}
%       \item \textbf{Breadth vs. Depth:} Compare the scores. Does \textit{Set-Balance} offer a statistically significant improvement over the extremes?
%       \item \textbf{Low-Resource Performance:} How much did quality degrade in the \textit{Set-Breadth} scenario?
% \end{itemize}

% \subsubsection{Impact of Architecture}
% \begin{itemize}
%       \item \textbf{AR vs. NAR:} Direct comparison of Tacotron~2 vs. FastPitch scores within the same data strategies. Which model was more robust to data sparsity?
% \end{itemize}

% \subsection{Summary of Findings}
% A brief paragraph summarizing the key takeaway (e.g., "FastPitch trained on the
% Balanced subset yielded the highest subjective naturalness, suggesting that
% 22.5 minutes per speaker is sufficient for high-quality synthesis...").

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

\subsection{Summary of findings}

\subsection{Contributions}

\subsection{Limitations of the study}

\subsection{Future work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\phantom{Appendix} References}

\printbibliography[heading=none]

% Examples are also provided for ChatGPT citation, both in general \cite{chatgpt_bendrai} and for a specific conversation \cite{chatgpt_pokalbis}.

\section{\phantom{Appendix} Appendix: Audio preprocessing parameters} \label{appendix:audio_params}

The following Mel-spectrogram extraction parameters were used by all models
(Tacotron~2, FastPitch, and HiFi-GAN~v2 vocoder):

\begin{itemize}
      \item fft\_size: 1024
      \item win\_length: 1024
      \item hop\_length: 256
      \item frame\_length\_ms: null
      \item frame\_shift\_ms: null
      \item stft\_pad\_mode: "reflect"
      \item sample\_rate: 22050
      \item resample: false
      \item preemphasis: 0.98
      \item ref\_level\_db: 20
      \item do\_sound\_norm: false
      \item log\_func: "np.log10"
      \item do\_trim\_silence: true
      \item trim\_db: 60
      \item do\_rms\_norm: false
      \item db\_level: null
      \item power: 1.5
      \item griffin\_lim\_iters: 60
      \item num\_mels: 80
      \item mel\_fmin: 0.0
      \item mel\_fmax: 8000.0
      \item spec\_gain: 20
      \item do\_amp\_to\_db\_linear: true
      \item do\_amp\_to\_db\_mel: true
      \item pitch\_fmax: 640.0
      \item pitch\_fmin: 1.0
      \item signal\_norm: true
      \item min\_level\_db: -100
      \item symmetric\_norm: true
      \item max\_norm: 4.0
      \item clip\_norm: true
      \item stats\_path: null
\end{itemize}

\end{document}
