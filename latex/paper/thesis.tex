%%%%%
%%%%%  Use LUALATEX, not LATEX.
%%%%%
%%%%
\documentclass[]{VUMIFTemplateClass}

\usepackage{indentfirst}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{mathtools}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage[hidelinks]{hyperref}
\usepackage{color,algorithm,algorithmic}
\usepackage[nottoc]{tocbibind}
\usepackage{tocloft}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage}

\makeatletter
\renewcommand{\fnum@algorithm}{\thealgorithm}
\makeatother
\renewcommand\thealgorithm{\arabic{algorithm} algorithm}

\usepackage{biblatex}
\bibliography{bibliografija}
%% to change the numbering (numeric or alphabetic) of bibliographic sources, make the change in VUMIFTemplateClass.cls, line 139

% Author's MACROS
\newcommand{\EE}{\mathbb{E}\,} % Mean
\newcommand{\ee}{{\mathrm e}}  % nice exponent
\newcommand{\RR}{\mathbb{R}}

\studyprogramme{Data Science}
\worktype{Master's thesis}
\worktitle{Data Selection Strategies for Multi-Speaker Text-to-Speech Synthesis in Lithuanian}
\secondworktitle{Work Title in Lithuanian}
\workauthor{Aleksandr Jan Smoliakov}

\supervisor{Gerda Ana Melnik-Leroy}
\reviewer{pedagogical/scientific title Name Surname}
%If present, otherwise delete
\scientificadvisor{pedagogical/scientific title Name Surname}
%If present, otherwise delete

\begin{document}
\selectlanguage{english}

\onehalfspacing
\input{TitlePage}

% %% TODO Acknowledgements Section
% \sectionnonumnocontent{Acknowledgements}
% The author is thankful the Information Technology Research Center, Faculty of Mathematics and Informatics, Vilnius University, for providing the High-Performance Computing (HPC) resources for this research.
% %%
% %%
% %%      If you have used IT resources (CPU-h, GPU-h, other IT resources) provided by MIF for your thesis research, please leave the acknowledgement; if you have not, you can delete it.
% %%
% %%

% You can also add here acknowledgements for various other things, such as your supervisor, university, company, etc.

\singlespacing
\selectlanguage{english}

% list of figures, delete if not needed
\listoffigures

% list of tables, delete if not needed
\listoftables

% Table of contents
\tableofcontents
\onehalfspacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\sectionnonum{Introduction}

The goal of creating machines that can speak like humans has captivated
researchers for centuries. One of the earliest known attempts dates back to the
18th century, with Wolfgang von Kempelen's mechanical speech machine that
utilized a bellows-driven lung and physical models of the tongue and lips.

Over the centuries, advancements in technology and understanding of human
speech have driven significant progress in this field. Today's state-of-the-art
systems, dominated by end-to-end (E2E) neural models, have achieved highly
naturalistic speech with unprecedented acoustic quality. Notably, these
end-to-end systems have unified the entire synthesis process into a single
neural network, eliminating the need for complex multi-stage pipelines.

% \subsection{Background and motivation}

% \subsection{Problem statement}

Training high-quality TTS models typically requires large amounts of annotated
speech data. The common recommendation is to use at least 10~hours of recorded
speech from a single speaker to achieve good results.

Liepa~2 is a recently released Lithuanian speech corpus that contains
1000~hours of annotated speech; however, this data is distributed across more
than 2600~speakers, with most speakers contributing only a few minutes of
speech. The top speaker has around 2.5~hours of recorded speech.

Training a high-quality single-speaker TTS model on such limited data poses a
challenge. Multi-speaker TTS models can utilize data from multiple speakers to
improve performance. However, training on all available data is a
time-consuming and computationally expensive process, especially in the context
of a master's thesis.

Therefore, it makes sense to explore strategies for selecting smaller subsets
of the available data for training. The question that arises is, what is the
best way to sample multi-speaker data for training TTS models?

% \subsection{Research questions}

This thesis aims to answer the following research questions:

\begin{itemize}
      \item TODO
\end{itemize}

% \subsection{Objectives}

% \subsection{Scope of the study}

Scope: This study is exclusively focused on the Lithuanian language and the
Liepa~2 speech corpus. It investigates a fixed total training data size of
30~hours. The models are limited to Tacotron~2 with DDC and FastPitch
architectures within the Coqui TTS framework, using a pre-trained WaveGlow
vocoder for waveform generation.

Limitations: The findings may not generalize to other languages, datasets with
different characteristics, or other TTS architectures. The 30-hour training
data size is a practical constraint and may not reflect performance at larger
scales.

% \subsection{Thesis structure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Literature review
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Literature review}

\subsection{Digital representation of audio}

Speech, or sound in general, is a continuous pressure wave that propagates
through a medium, such as air. The key properties of sound waves include
frequency (pitch), amplitude (loudness), and phase.

Converting continuous sound waves into a digital format suitable for computer
processing involves two main steps: sampling and quantization.

Sampling is the process of measuring the amplitude of the sound wave at regular
time intervals. The rate at which these samples are taken is called the
sampling rate. According to the Nyquist-Shannon~\cite{shannon1949communication}
sampling theorem, accurate reconstruction of a continuous signal requires a
sampling rate that is at least twice the highest frequency present in the
signal. The majority of human speech information is concentrated between 100~Hz
and 8~kHz.~\cite{rabiner2010theory} Thus, in text-to-speech applications,
common sampling rates for audio are 22.05~kHz and 24~kHz, which can capture
frequencies up to approx. 11~kHz and 12~kHz, respectively.

Quantization (also known as bit depth) is the mapping of continuous amplitude
values to discrete levels for digital representation. Typical bit depths for
audio are 16-bit and 24-bit formats.

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.8\textwidth]{figures/sampling_quantization.pdf}
      \caption{Visual representation of Analog-to-Digital conversion. The continuous grey line represents the analog signal. The vertical lines represent the \textbf{sampling rate} (time intervals), and the horizontal grid lines represent \textbf{quantization levels} (bit depth).}
      \label{fig:sampling}
\end{figure}

Pre-emphasis is a high-frequency filtering technique applied to audio signals
before further processing. Natural speech signals tend to have more energy in
the lower frequencies, with a gradual drop-off towards higher frequencies
(typically around -6~dB per octave). Pre-emphasis compensates for this spectral
tilt by boosting high frequencies using a first-order high-pass filter, which
is defined as:

\begin{equation}
      y[n] = x[n] - \alpha x[n-1]
\end{equation}

where \( y[n] \) is the pre-emphasized signal, \( x[n] \) is the original
signal, \( \alpha \) is the pre-emphasis coefficient (typically between 0.9 and
1.0), and \( n \) is the sample index.

This transformation balances the frequency spectrum, improving the
signal-to-noise ratio for higher frequencies and preventing the model from
optimizing only for low-frequency components.

\subsection{Time-Frequency Analysis}

\subsubsection{Fourier Transform}

Fourier Transform (FT) is a mathematical technique that transforms a
time-domain signal (such as an audio waveform) into its frequency-domain
representation. The signal is decomposed into a sum of sine and cosine waves at
various frequencies, each with a specific amplitude and phase. This allows us
to analyze the frequency content of the signal.

Short-Time Fourier Transform~\cite{oppenheim1999discrete} (STFT) extends the FT
by applying it to short, overlapping segments (frames) of the signal. This
transformation provides a time-frequency representation, showing how the
frequency content of the signal changes over time.

In TTS applications, the STFT is computed by dividing the audio signal into
short frames (usually, 20-50~ms) with a certain overlap (usually, 50-75\%)
between frames, windowed by a Hamming or Hann function to minimize the spectral
leakage.

\subsubsection{Spectrogram and Mel-spectrogram}

The spectrogram is a visual representation of the STFT, displaying frequency on
the vertical axis, time on the horizontal axis, and amplitude represented by
the color intensity.

However, the human ear does not perceive frequencies linearly --- it is more
sensitive to lower frequencies than higher ones. To mimic this perceptual
characteristic, the Mel scale~\cite{stevens1937scale} maps linear frequency \(
f \) (in Hz) to a perceptual scale \( m \) (in Mels) using the following
formula:

\begin{equation}
      m = 2595 \cdot \log_{10}\left(1 + \frac{f}{700}\right)
\end{equation}

Mel-spectrograms are computed by applying a Mel filterbank of overlapping
triangular filters (or kernels) to the magnitude spectrogram obtained from the
STFT. This results in a compressed representation of the audio signal that
aligns more closely with human auditory perception. Such Mel-spectrograms are
commonly used as input features for modern TTS systems.

\begin{figure}[ht]
      \centering
      % \includegraphics[width=0.6\textwidth]{figures/waveform_spectrograms.pdf}
      \caption{Raw audio waveform (top), its spectrogram (middle), and Mel-spectrogram (bottom) representations.}
      \label{fig:waveform_spectrograms}
\end{figure}

\subsection{Text-to-speech synthesis}

Text-to-Speech (TTS) synthesis, also known as speech synthesis, is the process
of converting written text into human-like spoken words. Nowadays TTS is a key
technology in numerous applications, including virtual assistants,
accessibility tools, and language learning platforms.

\subsubsection{Traditional TTS approaches}

Early attempts at artificial speech synthesis evolved from the first mechanical
devices in the 18th century to electronic systems. Wolfgang von Kempelen's
mechanical speech machine demonstrated basic phoneme production using a
physical model of the vocal tract. In 1937, Homer Dudley's invention of the
Voder~\cite{klatt1987review} became the first electronic speech synthesizer
that could produce intelligible speech through operator-controlled acoustic
parameters, establishing the foundation for modern electronic synthesis
methods.

In the decades that followed, two main approaches for speech synthesis emerged:
concatenative synthesis and parametric synthesis.

\subsubsection{Concatenative synthesis}

The concatenative synthesis approach~\cite{hunt1996unit} synthesizes speech by
piecing together pre-recorded segments of human speech. This method involves
several steps. First, it requires pre-recording a large database of speech
segments spoken by a human voice actor in pristine, highly controlled studio
conditions to ensure consistent audio quality and minimize background noise.
Each segment is labeled and indexed based on its phonetic and prosodic
properties.

During synthesis, the system breaks down the input text into short linguistic
units (such as phonemes or syllables) using a text analysis module. Then, it
queries the speech database to find the best-matching segments for each unit
using selection cost functions~\cite{black1995optimising}. The retrieved
segments are blended and concatenated to form a continuous speech waveform. The
system then uses signal processing techniques to smooth the transitions between
segments and adjust pitch and duration to match the desired output
characteristics.

Concatenative synthesis can produce natural-sounding individual speech units,
but the final audio often has noticeable glitches and breaks at the points
where segments are joined together~\cite{black1995optimising}. The segments may
not blend smoothly due to differences in pitch, duration, and timbre. The
prosody also tends to sound ``choppy'' and unnatural, since stringing
disjointed segments together does not capture the natural rhythm and intonation
patterns of connected speech.

Finally, concatenative synthesis requires language-specific expertise to design
and maintain the underlying speech database and selection algorithms. This need
for extensive data can make it challenging to develop concatenative TTS systems
for low-resource languages or dialects.

\subsubsection{Parametric synthesis}

In contrast, statistical parametric speech synthesis~\cite{zen2009statistical}
(SPSS) uses statistical models, typically Hidden Markov Models
(HMMs)~\cite{tokuda2013speech}, to generate the parameters that control a
speech waveform.

This method involves training a statistical model on a large corpus of recorded
speech. The model learns the relationship between linguistic features (like
phonemes and prosody) and the acoustic features of the speech signal, such as
spectral envelope and fundamental frequency. During synthesis, the system takes
text as input, converts it to a sequence of linguistic features, and then uses
the trained model to generate a corresponding sequence of acoustic parameters.

Compared to concatenative synthesis, the statistical approach allows for more
flexibility and control over the speech synthesis process, enabling the
generation of a wider variety of voices and speaking styles. However, HMM-based
synthesis~\cite{tokuda2013speech} had a persistent problem: the statistical
averaging built into the models tended to over-smooth the acoustic features,
creating a characteristic ``buzzy'' or ``muffled'' sound that lacked the
sharpness and detail of natural human speech.

\subsection{Linguistic Representation (Text Processing)}

\subsubsection{Text normalization}

Text normalization~\cite{sproat2001normalization} is the process of converting
raw text into a standardized format suitable for TTS synthesis. Typical steps
include expanding abbreviations (e.g., expanding ``Dr.'' to ``Doctor''),
punctuation removal, number normalization (e.g., converting ``123'' to ``one
hundred twenty-three''), and lowercasing.

\subsubsection{Graphemes vs. Phonemes}

A key decision in TTS systems is the choice of input representation for text.

There are two main approaches, namely grapheme-based and phoneme-based. The
grapheme-based approach uses the raw characters of the written text as input to
the TTS model. This method is straightforward and does not require any
additional pre-processing steps. However, it can struggle with languages that
have complex orthographies or inconsistent spelling-to-sound correspondences.

In contrast, the phoneme-based approach uses a phonetic transcription of the
text, typically in the International Phonetic Alphabet (IPA) format. The
underlying assumption is that phonemes possess a more direct and consistent
mapping to the acoustic features of speech, making it easier for the TTS model
to learn the relationship between text and audio. However, this approach
requires an additional grapheme-to-phoneme (G2P) conversion
step~\cite{bisani2008joint}, which requires a complex pre-processing pipeline
or a separate G2P model.

There is another approach that augments the grapheme-based representation with
accentuation marks. This method adds accent marks (typically tilde, acute,
grave accents) to the raw grapheme sequence to indicate the stressed syllables
in words. This additional information provides cues about the prosodic features
of speech, which can help improve the pronunciation and naturalness of the
synthesized speech.

\subsubsection{Specific challenges in Lithuanian}

Lithuanian is a Baltic language with a rich inflectional morphology and complex
prosodic structure. It is a pitch-accent language with free stress, meaning the
stress can fall on any syllable in a word, and can change the position
depending on the grammatical form.

Challenges in Lithuanian TTS synthesis include:

\begin{itemize}
      \item \textbf{High OOV rate:} Due to extensive word inflection, the number of unique word forms is significantly higher than in English. This leads to data sparsity issues where many valid word forms may not appear in the training set.
      \item \textbf{Ambiguity without accentuation:} Typically, stress marks are omitted in written Lithuanian. However, stress position and tone (acute, circumflex, or short) determine the meaning of monographic words. Examples are shown in Table~\ref{tab:lithuanian_ambiguity}. A grapheme-based model without accentuation marks may struggle to predict the correct prosody, resulting in monotonic or mispronounced speech.
\end{itemize}

\begin{table}[ht]
      \centering
      \begin{tabular}{|l|c|c|}
            \toprule
            \textbf{Word}          & \textbf{Accentuation}       & \textbf{Meaning}      \\
            \midrule
            \multirow{2}{*}{Antis} & \textit{ántis} (Acute)      & A duck (noun)         \\
                                   & \textit{añtis} (Circumflex) & Bosom/Chest (noun)    \\
            \midrule
            \multirow{2}{*}{Kasa}  & \textit{kãsa} (Circumflex)  & He/she digs (verb)    \\
                                   & \textit{kasà} (Short)       & Braid/Pancreas (noun) \\
            \bottomrule
      \end{tabular}
      \caption{Examples of Lithuanian homographs where accentuation determines meaning. A grapheme-only model cannot distinguish these without context or explicit stress marks.}
      \label{tab:lithuanian_ambiguity}
\end{table}

To overcome these challenges, tools like \textbf{Kirčiuoklis} (Vytautas Magnus
University) are often employed in the text normalization pipeline. Kirčiuoklis
automatically assigns stress marks to raw text. One weakness of Kirčiuoklis is
that it relies on a word-dictionary based approach, which does not take into
account the context of the word. Thus, it suggests multiple possible
accentuation variants for ambiguous words, leaving it up to the user to select
the correct one.

In the absence of a high-quality, context-aware Grapheme-to-Phoneme (G2P)
converter for Lithuanian, this thesis will focus on grapheme-based TTS
synthesis with accentuation marks provided by Kirčiuoklis. In cases where
Kirčiuoklis suggests multiple accentuation variants for a word, no stress marks
will be added, leaving the TTS model to infer the correct prosody from context.

\subsection{Embeddings and Representation Learning}

\subsubsection{The Concept of Embeddings}

In machine learning, embeddings are dense vector representations of discrete
entities (such as words, characters, or speakers) to a high-dimensional
continuous vector space. Unlike one-hot encodings, which are sparse and highly
dimensional, embeddings provide a dense, lower-dimensional representation that
captures semantic relationships between underlying entities. For instance, in
word embeddings, similar words tend to have more similar (correlated) vector
representations, while dissimilar words map to more distant points in the
vector space.

\subsubsection{Text Embeddings}

The ``Encoder'' part of a TTS model is responsible for converting a sequence of
input symbols (characters or phonemes) into a sequence of feature vectors.
Typically, this is done using an embedding layer, which maps each input symbol
to a learnable fixed-size vector representation. During training, these
embeddings are learned jointly with the rest of the TTS model.

\subsubsection{Speaker Embeddings}

In order to enable multi-speaker synthesis, TTS models need to receive a
representation of the speaker's identity.

\textbf{Lookup Tables (LUT):} Early approaches used simple learnable embeddings where each speaker ID is
mapped to a unique vector. The vectors are learned joinly with the TTS model
during training. While efficient, this approach cannot generalize to speakers
not seen during training.

\textbf{Reference encoders and speaker embeddings:}

\begin{figure}[ht]
      \centering
      % \includegraphics[width=\textwidth]{figures/speaker_encoder_diagram.pdf}
      \caption{General architecture of a Speaker Encoder. A reference audio of arbitrary length is processed (typically by LSTM or TDNN layers) and pooled to produce a fixed-length embedding vector (e.g., d-vector) representing the speaker identity.}
      \label{fig:speaker_encoder}
\end{figure}

Transfer learning approaches~\cite{jia2018transfer} have demonstrated adapting
speaker verification models to multispeaker TTS synthesis, enabling better
speaker adaptation and higher voice quality. A speaker encoder model
pre-trained on a massive, noisy dataset with thousands of speakers (such as
VoxCeleb) learns the general speaker space. Its pre-trained weights are then
frozen and used to extract embeddings for the TTS training data, allowing the
TTS model to effectively account for multi-speaker characteristics.

\textbf{d-vectors:} d-vectors~\cite{variani2014deep} are fixed-length speaker embeddings derived from a separate speaker verification model. A reference encoder network takes a reference audio recording of arbitrary length and compresses it into a fixed-length vector known as a d-vector, that summarizes the speaker's timbral and prosodic characteristics. These d-vectors are then provided as additional input to the TTS model, and are kept fixed during TTS training.

\textbf{x-vectors:} An evolution of d-vectors, x-vectors~\cite{snyder2018x} use a more Time Delay Neural Network (TDNN) architecture to capture the temporal context more effectively. These embeddings have shown an improved ability in zero-shot TTS scenarios.

One limitation of d-vectors and x-vectors is that if the reference audio is of
poor quality or contains background noise, the resulting speaker embedding may
not accurately represent the speaker's identity, leading to degraded synthesis
quality.

% TODO How Coqui TTS handles speakers: Specifically, how speaker embeddings are
% concatenated or added to the encoder outputs to condition the synthesis on a
% specific voice identity.

\subsection{Deep learning for TTS}

The limitations of complex, multi-stage pipelines motivated the creation of a
new approach, the end-to-end (E2E) model. E2E systems learn the entire speech
synthesis process --- from input text directly to acoustic output --- using a
single neural network. This approach promised to eliminate the need for
hand-crafted pipelines that were difficult to design, required extensive
expertise, and suffered from errors that accumulated across multiple
components. By learning directly from text-audio pairs, E2E models showed they
could produce speech with higher naturalness and expressiveness than previous
methods, representing a significant leap in TTS technology.

However, although deep learning TTS models are more robust to variations in
data quality compared to concatenative approaches, they are essentially
``data-hungry'' beasts that require large amounts of training data to achieve
optimal performance. This relationship between model performance and training
data size follows well-established scaling laws for neural
models~\cite{kaplan2020scaling}. This characteristic makes data selection and
quality control critical factors in developing effective neural TTS systems.

\subsubsection{Feedforward neural networks}

Feedforward Neural Networks (FNNs) are the simplest type of artificial neural
networks, consisting of layers of interconnected nodes (neurons) where
information flows in one direction --- from the input, through hidden layers,
to the output. While FNNs can be useful for basic regression or classification
tasks, they lack the memory and context-awareness needed for processing
sequential data like text and speech. Therefore, FNNs are not suitable for
modelling TTS tasks that require understanding of temporal dependencies.

\subsubsection{Encoder-Decoder architectures}

The Encoder-Decoder architecture is a neural network architecture consisting of
two components, namely an encoder and a decoder. The encoder processes the
input data and compresses it into a high-dimensional latent representation.
This vector captures the meaningful features of the input. The decoder uses
this latent representation as context to generate the final output. This
architecture is commonly used in sequence-to-sequence tasks, such as machine
translation (text-to-text) and text-to-speech synthesis (text-to-audio frames).

\subsubsection{Tacotron~2}

A common approach for TTS modelling it the sequence-to-sequence (seq2seq)
framework, which uses an encoder-decoder architecture with an attention
mechanism to map input (text) sequences to output (audio) sequences.

This architecture has three main components:

\textbf{Encoder:} The encoder processes through the input text sequence (graphemes or phonemes) and converts it into a high-level representation. Typically, this is done using recurrent neural networks (RNNs) or more complex modules like the CBHG (Convolution Bank + Highway network + Gated Recurrent Unit).
\textbf{Attention Mechanism:} The attention mechanism learns to align the high-level text representation with the output audio frames. At each decoding step, it determines which part (positions) of the input text the model should focus (or ``attend to'') in order to generate the next audio frame.
\textbf{Decoder:} Finally, the decoder --- also typically an RNN --- takes the encoder output, uses the attention output as context, and generates an acoustic representation of speech (typically mel-spectrogram) frame by frame. Being autoregressive, the decoder is a ``bottleneck'' during inference, as it must generate each frame sequentially, only after the previous one has been produced.

\begin{figure}[ht]
      \centering
      % \includegraphics[width=\textwidth]{figures/tacotron2_arch.pdf}
      \caption{The Tacotron 2 architecture. Note the recurrent connections in the decoder and the attention mechanism aligning encoder outputs to decoder steps. \cite{shen2018natural}}
      \label{fig:tacotron_arch}
\end{figure}

Tacotron~\cite{wang2017tacotron} and Tacotron~2~\cite{shen2018natural} are two
notable TTS models based on the sequence-to-sequence architecture. The variant
that this thesis primarily focuses on is Tacotron~2 with Double Decoder
Consistency (DDC).

In Tacotron~2, the encoder consists of a character embedding layer, followed by
a stack of convolutional layers (which encode local context about neighbouring
characters), and a bidirectional LSTM. The attention mechanism is
location-sensitive, computing an alignment between the encoder outputs (text)
and the decoder steps (audio frames). The decoder is an autoregressive RNN,
typically composed of LSTM layers.

\subsubsection{FastPitch}

\begin{figure}[ht]
      \centering
      % \includegraphics[width=\textwidth]{figures/fastpitch_arch.pdf}
      \caption{The FastPitch architecture. It utilizes a feed-forward Transformer and an explicit duration predictor, allowing for parallel generation of the Mel-spectrogram. Pitch is predicted and injected into the latent representation. \cite{lancucki2020fastpitch}}
      \label{fig:fastpitch_arch}
\end{figure}

Unlike Tacotron~2, FastPitch~\cite{lancucki2020fastpitch} replaces the
recurrent layers with Transformer architecture blocks relying on
self-attention. Being a non-autoregressive model, FastPitch model can process
the entire input sequence in parallel, significantly speeding up both training
and inference.

Unlike Tacotron~2, which learns alignment implicitly using attention, FastPitch
requires an external aligner (or an internal alignment module) in order to
determine how many sound frames correspond to each input character. A duration
predictor network learns to predict these durations, ensuring the speech rhythm
is stable.

FastPitch includes a dedicated predictor that estimates the fundamental
frequency (F0) for every input symbol. The predicted pitch values are projected
and added to the hidden states, allowing the model to explicitly control
intonation before generating the spectrogram.

The primary advantages are inference speed (duet to non-autoregressive
generation), robustness (no attention failures like skipping or repeating
words), and controllability (pitch and speed can be tweaked manually during
synthesis).

\subsubsection{Notable End-to-End TTS models}

Besides Tacotron~2 and FastPitch, other notable TTS architectures include
\textbf{Glow-TTS}, which uses flow-based generative models for parallel
inference, and \textbf{VITS} (Conditional Variational Autoencoder with
Adversarial Learning), which combines the acoustic TTS model (Glow-TTS) with a
neural vocoder (HiFi-GAN) into a single end-to-end architecture.

\subsubsection{Neural vocoders}

Standard TTS models like Tacotron~2 or FastPitch generate intermediate acoustic
features (typically mel-spectrograms), but do not directly produce raw audio
waveforms. However, mel-spectrograms are lossy representations that only
capture the magnitude of the frequency bands, discarding phase information.
This information is discarded during the STFT process. Thus, converting a
spectrogram into audio is a non-trivial task, as the phase information must be
estimated. This challenge is known as the \textit{inversion problem}.

The tool used to convert spectrograms back into waveforms is called a vocoder.

Traditionally, the Griffin-Lim algorithm~\cite{griffin1984signal} has been used
to iteratively estimate and reconstruct the phase information from the
magnitude spectrogram. However, this method often produces audio with
noticeable artifacts and lower quality compared to natural speech.

Neural GAN (Generative Adversarial Network) based vocoders, such as
HiFi-GAN~\cite{kong2020hifi}, have become the dominant approach for vocoding in
modern TTS systems. These GAN-based vocoder models consist of two main
components: a Generator, which reconstructs mel-spectrograms into raw
waveforms, and a Discriminator, which distinguishes between real and
synthesized audio. This adversarial training forces the Generator to produce
high-fidelity audio that closely resembles natural speech.

The framework used in this thesis, Coqui TTS~\cite{coqui2021}, comes with a pre-trained HiFi-GAN v2 vocoder trained on a large
multi-speaker dataset (VCTK). Given its widespread adoption and status as a
state-of-the-art neural vocoder, this thesis will use the HiFi-GAN v2 model for
waveform generation. In order to ensure compatibility between the TTS models
and the vocoder, the TTS models' acoustic parameters will be configured to the
exact same settings (sampling rate, FFT parameters, Mel filterbank settings) as
those used during the HiFi-GAN v2 training.

\subsection{Multi-speaker TTS}

Multi-speaker TTS models are designed to synthesize speech in the voices of
multiple speakers. In order to achieve this, these models are trained on data
from many different speakers, allowing them to learn the characteristics of
each voice and synthesize speech that sounds like a specific individual, while
still being able to generalize the shared linguistic and acoustic patterns
across speakers.

\subsubsection{Techniques}

Techniques for multi-speaker TTS often involve conditioning the model on
speaker identity. This can be done by providing a speaker embedding vector as
an additional input to the model. Early successful implementations of this
approach include Deep Voice 2~\cite{gibiansky2017deep}, which demonstrated
effective multi-speaker synthesis by learning speaker-specific embeddings.

The speaker embedding can be learned jointly with the TTS model during
training, or it can be pre-trained using a separate speaker verification model.

\subsubsection{Challenges}

One key challenge in multi-speaker TTS is ensuring that the model can
effectively capture the unique characteristics of each speaker's voice while
still producing natural-sounding speech.

The distribution of training data across speakers can also influence the
model's performance and its ability to mimic all voices equally well. The
training datasets are often strongly imbalanced, with some speakers having
hours of annotated speech, while others may have only a few minutes. This
imbalance can cause the model to overfit to the speakers with more data,
resulting in lower synthesis quality for underrepresented speakers.

Thus, data selection and sampling strategies are critical considerations when
training multi-speaker TTS models.

\subsection{Data selection in TTS}

As TTS models require large amounts of annotated speech data for training,
training on all available data can be computationally prohibitive and
time-consuming. Therefore, data selection aims to identify optimal subsets of
the available data that can yield high-quality synthesis while minimizing
training time and resource requirements.

\subsubsection{Data quantity}

The relationship between the quantity of training data and model performance in
neural networks follows predictable scaling laws. Research by Kaplan et
al.~\cite{kaplan2020scaling} showed that model performance generally improves
as a power law function of the amount of training data, with diminishing
returns at larger scales. Similar trends have been observed in TTS models,
where increasing the training data size leads to better synthesis quality and
naturalness, but with diminishing improvements as the dataset grows larger.

While this would suggest that ``more data is better'', in practice, collecting,
curating, and processing large-scale multi-speaker TTS training datasets, as
well as the computational burden of training on them, can be prohibitive. This
makes efficient data selection strategies increasingly valuable for achieving
optimal performance-to-cost ratios.

\subsubsection{Data selection strategies}

% TODO

\subsection{Evaluation methods for TTS}

\subsubsection{Subjective evaluation}

TTS systems are often evaluated using subjective listening tests, where human
listeners rate the naturalness and intelligibility of synthesized speech
samples.

A common subjective evaluation method is the Mean Opinion Score~\cite{itup800}
(MOS) test. In a MOS test, listeners are presented with a set of synthesized
speech samples and are asked to rate each sample on a scale from 1 to 5. Here,
the scale typically represents the following levels of quality:

\begin{itemize}
      \item 1 - Bad: The speech is completely unnatural and unintelligible.
      \item 2 - Poor: The speech is mostly unnatural and difficult to understand.
      \item 3 - Fair: The speech is somewhat natural but has noticeable artifacts or
            issues.
      \item 4 - Good: The speech is mostly natural with minor artifacts that do not
            significantly affect intelligibility.
      \item 5 - Excellent: The speech is indistinguishable from natural human speech.
\end{itemize}

\subsubsection{Objective evaluation}

Objective evaluation methods use computational metrics to assess the quality of
synthesized speech.

Common objective metrics include:

\begin{itemize}
      \item Mel-Cepstral Distortion~\cite{kubichek1993mel} (MCD): Measures the spectral
            distance between synthesized and natural speech.
      \item Fundamental Frequency Root Mean Square Error (F0 RMSE): Assesses the accuracy
            of pitch contours.
\end{itemize}

\subsection{Summary}

Modern TTS synthesis has evolved from complex, multi-stage concatenative and
statistical pipelines to end-to-end deep learning architectures. Models such as
Tacotron~2 and FastPitch, combined with neural vocoders like HiFi-GAN, are
capable of generating natural-sounding speech that is often indistinguishable
from human speech. These systems rely on digital signal processing techniques
to convert raw audio into representations like Mel-spectrograms --- and learned
embeddings to handle text-to-audio alignment and speaker identity.

However, these deep learning models are notoriously data-hungry, requiring
large amounts of high-quality, annotated speech. While English benefits from
massive, well-curated datasets (e.g., LJSpeech, VCTK), low-resource languages
face distinct challenges. Specifically, Lithuanian possesses a complex prosodic
structure characterized by free stress and pitch accents, requiring accurate
text normalization and accentuation handling. Furthermore, multi-speaker
synthesis requires the model not only to learn these linguistic features but
also to generalize across a latent speaker space, typically managed via
d-vector or x-vector embeddings.

\subsection{Research Gap}

% Despite the advancements in model architectures, a significant gap remains in
% the literature regarding \textbf{data selection strategies for highly
%       fragmented, imbalanced corpora} in low-resource settings.

% Most current state-of-the-art multi-speaker TTS research relies on datasets
% that are either:
% \begin{enumerate}
%       \item \textbf{Single-speaker high-resource:} Hundreds of hours of studio-quality speech from a single professional voice actor.
%       \item \textbf{Multi-speaker balanced:} Curated datasets like VCTK, where roughly 100 speakers have a similar amount of clean data.
% \end{enumerate}

The Liepa~2 corpus, which serves as the foundation for this thesis, is a
realistic scenario common for lesser-resourced languages: it contains a massive
number of speakers (2621) but is highly imbalanced, with data per speaker
ranging from a few seconds to over two hours.

Current literature does not adequately address how to optimally sample from
such a distribution under strict computational constraints. While scaling
laws~\cite{kaplan2020scaling} suggest that increasing total dataset size
improves performance, training on the full 1000-hour corpus is computationally
prohibitive for many academic and industrial applications. Consequently,
researchers must select a subset of data (e.g., 30 hours), but it is unclear
which selection strategy gives the best synthesis quality:

\begin{itemize}
      \item Does maximizing the \textit{speaker depth} (selecting the ``Top-N'' speakers
            with the most data) allow the model to learn stable alignments and prosody
            better?
      \item Or does maximizing the \textit{speaker breadth} (selecting a ``Balanced'' or
            ``Random'' set of many speakers with less data per person) allow the model to
            learn a more robust, generalized representation of the language's phonetics?
\end{itemize}

This thesis addresses this gap by evaluating different data selection
strategies for training multi-speaker TTS models on the Lithuanian Liepa~2
corpus. By keeping the total training data budget fixed (30 hours) and varying
the selection logic (Top-N vs. Random vs. Balanced) across autoregressive
(Tacotron~2) and non-autoregressive (FastPitch) architectures, this research
aims to provide practical guidelines for training high-quality TTS systems in
resource-constrained, high-speaker-count scenarios.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Methodology
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

\subsection{Research design}

The independent variables in this study are:

\begin{itemize}
      \item Data selection strategy: Different methods for selecting subsets of the
            training data.
      \item TTS model architecture: Comparing different TTS architectures.
\end{itemize}

The dependent variables in this study are:

\begin{itemize}
      \item TTS model performance: Measured using objective metrics (TODO, decide) and
            subjective evaluations (MOS scores).
\end{itemize}

The controlled variables in this study are:

\begin{itemize}
      \item Dataset: The Liepa~2 Lithuanian speech corpus.
      \item Training data size: The same total amount of training data used across all
            experiments.
      \item Training procedure: The same training hyperparameters and protocols applied
            across all experiments.
      \item Evaluation metrics: The same objective and subjective evaluation methods
            applied across all experiments.
\end{itemize}

\subsection{Data and preprocessing}

\subsubsection{Liepa~2 dataset}

The primary dataset for this research is the Liepa~2 corpus, a comprehensive
collection of Lithuanian speech data. The corpus encompasses 1000~hours of
short voice recordings from 2621~speakers, along with corresponding text
transcriptions. The recordings span various speech styles and contexts,
including read speech (audiobooks, news), TV and radio broadcasts, spontaneous
speech.

More specifically, the dataset contains:
\begin{itemize}
      \item Total duration: 1000~hours of speech data.
      \item Number of speakers: 2621~unique speakers.
      \item Number of utterances: 1,874,648~recorded utterances.
\end{itemize}

\subsubsection{Acoustic preprocessing}

The raw audio recordings from the Liepa~2 dataset are sampled at 16~kHz. To
prepare the audio for TTS model training, the audio waveforms are resampled to
22,050~Hz. While resampling to a higher frequency does not add new information,
it will be compatible with pre-trained vocoders that expect 22,050~Hz input.

Additional preprocessing steps, such as silence trimming and normalization, are
performed by the Coqui TTS framework during training.

The acoustic processing that transforms audio waveforms into mel-spectrograms
is performed on-the-fly during model training by the Coqui TTS framework. The
mel-spectrogram parameters used are as follows:

TODO move to appendix

\begin{itemize}
      \item fft\_size: 1024
      \item win\_length: 1024
      \item hop\_length: 256
      \item frame\_length\_ms: null
      \item frame\_shift\_ms: null
      \item stft\_pad\_mode: "reflect"
      \item sample\_rate: 22050
      \item resample: false
      \item preemphasis: 0.98
      \item ref\_level\_db: 20
      \item do\_sound\_norm: false
      \item log\_func: "np.log10"
      \item do\_trim\_silence: true
      \item trim\_db: 60
      \item do\_rms\_norm: false
      \item db\_level: null
      \item power: 1.5
      \item griffin\_lim\_iters: 60
      \item num\_mels: 80
      \item mel\_fmin: 0.0
      \item mel\_fmax: 8000.0
      \item spec\_gain: 20
      \item do\_amp\_to\_db\_linear: true
      \item do\_amp\_to\_db\_mel: true
      \item pitch\_fmax: 640.0
      \item pitch\_fmin: 1.0
      \item signal\_norm: true
      \item min\_level\_db: -100
      \item symmetric\_norm: true
      \item max\_norm: 4.0
      \item clip\_norm: true
      \item stats\_path: null
\end{itemize}

\subsubsection{Text normalization}

Before feeding text data into TTS models, the text undergoes normalization to
convert it into a more consistent and model-friendly format. The Liepa~2 text
already includes a significant level of normalization - for instance, the
following elements are written exactly as they were spoken: dates, times,
acronyms, abbreviations, numbers.

However, some additional normalization steps are applied to further standardize
the text:

\begin{itemize}
      \item Punctuation standardization: Replacing uncommon punctuation marks with more
            common equivalents (e.g., replacing em dashes with hyphens, and semicolons with
            commas).
      \item Removal of extraneous characters: Eliminating any remaining characters that are
            not letters, digits, whitespace, or basic punctuation (.,-?!).
      \item Whitespace normalization: Collapsing multiple consecutive whitespace characters
            into a single space and trimming leading/trailing whitespace.
      \item Accent addition: Adding accent marks (tilde, acute, grave) to words using
            Kirčiuoklis tool (where Kirčiuoklis suggests multiple options, accent is not
            added).
      \item Lowercasing: Converting all text to lowercase to reduce vocabulary size.
      \item Letter replacements: Substituting non-Lithuanian letters with their Lithuanian
            equivalents (`w' with `v', `q' with `kv', and `x' with `ks').
\end{itemize}

In order to add the accents to the text, the online tool Kirčiuoklis was
queried with all unique words from the dataset. The tool returned the accented
versions of the words, which were then used to replace the unaccented words in
the text. In cases where Kirčiuoklis provided multiple accentuation options for
a word, no accent was added to avoid introducing errors.

As a result of these normalization steps, the vocabulary size is reduced from
140~characters to 41~characters, simplifying the learning task for TTS models.

\subsubsection{Speaker embeddings}

For multispeaker TTS training, speaker embeddings are computed using a
pre-trained speaker encoder model. The speaker embedding computation process
involves:

\begin{itemize}
      \item \textbf{Speaker encoder model}: Pre-trained model from Coqui TTS based on TODO
            % the approach described in~\cite{jia2018transfer}.
      \item \textbf{Embedding extraction}: Each speaker's audio samples are processed
            through the speaker encoder to generate 512-dimensional speaker embeddings
            (d-vectors).
      \item \textbf{Pooling strategy}: TODO
      % Global average pooling across all utterances
      %       from each speaker to create a single representative embedding per speaker.
      \item \textbf{Normalization}: TODO
      % Speaker embeddings are L2-normalized to ensure
      %       consistent magnitude across speakers.
\end{itemize}

These speaker embeddings are then used during TTS training to condition the
model on speaker identity, enabling the synthesis of speech in different
voices.

\subsection{Data selection strategies}

To investigate the impact of data selection on TTS performance, several
strategies for selecting subsets of the Liepa~2 dataset were employed. All
strategies maintain a fixed total training budget of 30~hours to ensure fair
comparison across experiments.

\subsubsection{Speaker filtering criteria}

Before applying selection strategies, the dataset undergoes initial filtering:
\begin{itemize}
      \item \textbf{Speech type filtering}: Only ``read speech'' samples are used, excluding
            spontaneous speech to ensure consistent quality and pronunciation.
      \item \textbf{Age group filtering}: Speakers from age groups 18--25, 26--60, and 60+
            are included, excluding children (0--17) to focus on adult speech patterns.
      \item \textbf{Gender balancing}: Equal representation of male and female speakers
            when possible.
\end{itemize}

\subsubsection{Selection strategies}

\textbf{Top-N speaker (Speaker depth):}
This strategy prioritizes speaker depth by selecting the N speakers with the
most available data. For multispeaker training with 20 speakers, the top 10
male and top 10 female speakers (by sample count) are selected. This approach
maximizes the amount of data per speaker, potentially allowing models to learn
more stable speaker-specific characteristics and alignment patterns.

\textbf{Balanced speaker (Speaker breadth):}
This strategy prioritizes speaker diversity by distributing the training budget
across a larger number of speakers, with each speaker contributing roughly equal
amounts of data. The selection process aims to include as many speakers as
possible while maintaining the 30-hour budget constraint and gender balance.

\textbf{Random sampling:}
As a baseline, this strategy randomly samples utterances from the filtered
dataset without regard to speaker distribution. This provides a control
condition to assess whether structured speaker selection provides benefits over
random data selection.

\subsection{Experimental design}

\subsubsection{Model architectures}

Three TTS model architectures are evaluated in this study:

\textbf{Tacotron~2 with Double Decoder Consistency (DDC):}
The primary autoregressive model used in this study. DDC is an enhanced version
of Tacotron~2 that employs dual decoders with different reduction factors (r=1
for the main decoder, r=7 for the coarse decoder) to improve attention
alignment stability. This architecture is particularly suitable for multispeaker
scenarios where attention alignment can be challenging.

% \textbf{Tacotron~2 with Dynamic Convolution Attention (DCA):}
% An alternative Tacotron~2 variant that replaces the standard location-sensitive
% attention with dynamic convolution attention mechanisms for potentially improved
% alignment performance.

\textbf{FastPitch:}
A non-autoregressive model that predicts duration and pitch explicitly, allowing
for parallel generation of mel-spectrograms. FastPitch uses Transformer
architecture instead of RNNs and should theoretically be less sensitive to
attention alignment issues.

\subsubsection{Training procedure}

All models are trained using the Coqui TTS framework with consistent training
procedures:

\begin{itemize}
      \item \textbf{Training duration}: 400 epochs maximum with early stopping based on
            validation loss.
      \item \textbf{Optimization}: RAdam optimizer with learning rate scheduling using
            MultiStepLR (milestones at 10k, 20k, 30k, 40k steps with gamma=0.32).
      \item \textbf{Loss function}: Combination of decoder loss (α=0.25), post-net loss
            (α=0.25), SSIM losses (α=0.25), guided attention loss (α=5.0), and stop token
            loss (weight=15.0).
      \item \textbf{Batch size}: 64 for Tacotron~2 variants, 32 for FastPitch.
      \item \textbf{Gradient clipping}: 0.05 to prevent gradient explosion.
      \item \textbf{Validation}: 1\% of training data held out for validation.
\end{itemize}

\subsection{Model training configurations}

\subsubsection{Tacotron~2 with DDC hyperparameters}

\begin{table}[h!]
      \centering
      \caption{Tacotron~2 DDC training configuration}
      \begin{tabular}{lr}
            \toprule
            \textbf{Parameter}       & \textbf{Value}     \\
            \midrule
            Batch size               & 64                 \\
            Learning rate            & 0.001              \\
            Optimizer                & RAdam              \\
            LR scheduler             & MultiStepLR        \\
            Max epochs               & 400                \\
            Decoder reduction factor & 1                  \\
            DDC reduction factor     & 7                  \\
            Attention type           & Location-sensitive \\
            Memory size              & -1 (disabled)      \\
            Speaker embedding dim    & 512                \\
            Number of speakers       & 20                 \\
            Stopnet                  & Enabled            \\
            Separate stopnet         & True               \\
            \bottomrule
      \end{tabular}
\end{table}

\subsubsection{FastPitch hyperparameters}

\begin{table}[h!]
      \centering
      \caption{FastPitch training configuration}
      \begin{tabular}{lr}
            \toprule
            \textbf{Parameter}         & \textbf{Value} \\
            \midrule
            Batch size                 & 32             \\
            Eval batch size            & 16             \\
            Learning rate              & 0.001          \\
            Optimizer                  & RAdam          \\
            LR scheduler               & MultiStepLR    \\
            Max epochs                 & 400            \\
            Duration predictor layers  & 2              \\
            Pitch predictor layers     & 2              \\
            Transformer encoder layers & 6              \\
            Transformer decoder layers & 6              \\
            Attention heads            & 1              \\
            Encoder hidden dim         & 384            \\
            Decoder hidden dim         & 384            \\
            \bottomrule
      \end{tabular}
\end{table}

\subsubsection{Vocoder configuration}

For mel-spectrogram to waveform conversion, a pre-trained HiFi-GAN v2 vocoder
is used across all experiments. The vocoder was pre-trained on the VCTK
corpus~\cite{veaux2019cstr} and is compatible with the 22.05~kHz sampling rate
used in this study. Using the same vocoder across all experiments ensures that
differences in audio quality can be attributed to the TTS model rather than the
vocoder.

\subsection{Evaluation methodology}

\subsubsection{Objective evaluation metrics}

Objective evaluation is performed using standard acoustic metrics:

\begin{itemize}
      % \item \textbf{Mel Cepstral Distortion (MCD):} Measures the spectral distance
      %       between synthesized and ground truth mel-spectrograms, with lower values
      %       indicating better quality.
      % \item \textbf{Fundamental Frequency RMSE (F0 RMSE):} Evaluates pitch accuracy
      %       by measuring the root mean square error between predicted and ground truth
      %       F0 contours.
      \item \textbf{Training convergence metrics:} Training and validation loss
            curves, attention alignment visualization, and convergence time.
\end{itemize}

\subsubsection{Subjective evaluation}

Subjective evaluation is conducted through a web-based listening test
application developed specifically for this study. The evaluation protocol
includes:

\begin{itemize}
      \item \textbf{Rating scale:} 5-point Mean Opinion Score (MOS) scale where
            1=Very Poor, 2=Poor, 3=Fair, 4=Good, 5=Excellent.
      % \item \textbf{Test sentences:} A standardized set of 50 Lithuanian sentences
      %       covering various phonetic contexts and prosodic patterns.
      % \item \textbf{Evaluation criteria:} Participants rate overall quality considering
      %       naturalness, intelligibility, and absence of artifacts.
      % \item \textbf{Participant recruitment:} Native Lithuanian speakers recruited
      %       through university networks and social media.
      % \item \textbf{Data collection:} Web-based application with PostgreSQL database
      %       for storing ratings, deployed on Google Cloud Platform.
      % \item \textbf{Statistical analysis:} Mann-Whitney U tests for significance
      %       testing between model comparisons, given the ordinal nature of MOS ratings.
\end{itemize}

\subsubsection{Computational hardware}

The experiments were conducted on a personal high-performance computing setup
with the following specifications:

\begin{itemize}
      \item CPU: AMD\@ Epyc 7642 48-Core, 96~thread processor
      \item RAM: 256 GB\@ DDR4 3200 MHz
      \item GPU: NVIDIA\@ GeForce RTX\@ 3090 with 24 GB\@ VRAM
      \item Storage: 2 TB\@ NVMe SSD
\end{itemize}

\subsubsection{Implementation framework}

All experiments are implemented using the Coqui TTS framework,
an open-source toolkit for training TTS models. The experimental pipeline is
automated using Make build system with the following components:

\begin{itemize}
      \item \textbf{Data preprocessing:} Automated scripts for audio conversion,
            text normalization, and metadata generation.
      \item \textbf{Speaker embedding computation:} Batch processing of speaker
            embeddings using pre-trained encoder models.
      \item \textbf{Training orchestration:} Automated model training with
            hyperparameter configuration and checkpointing.
      \item \textbf{Inference pipeline:} Batch synthesis of test sentences for
            evaluation purposes.
      \item \textbf{Evaluation tools:} Integration with subjective evaluation
            web application and objective metric computation.
\end{itemize}

\subsection{Evaluation protocol}

\subsubsection{Test set}

Evaluation was performed on a held-out test set of 100~phrases from the Liepa~2
dataset.

\subsubsection{Objective evaluation}

\subsubsection{Subjective evaluation}

Subjective evaluation was conducted using Mean Opinion Score (MOS) tests.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Results and analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and analysis}

\subsection{Tacotron~2 with DDC}

\subsubsection{Objective results}

\subsubsection{Subjective results}

\subsubsection{Qualitative analysis}

\subsection{FastPitch}

\subsubsection{Objective results}

\subsubsection{Subjective results}

\subsubsection{Qualitative analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

\subsection{Summary of findings}

\subsection{Contributions}

\subsection{Limitations of the study}

\subsection{Future work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\phantom{Appendix} References}

\printbibliography[heading=none]

% Examples are also provided for ChatGPT citation, both in general \cite{chatgpt_bendrai} and for a specific conversation \cite{chatgpt_pokalbis}.

\end{document}
